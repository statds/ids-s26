[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science",
    "section": "",
    "text": "Preliminaries\nThe notes are developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#acknowledgement",
    "href": "index.html#acknowledgement",
    "title": "Introduction to Data Science",
    "section": "Acknowledgement",
    "text": "Acknowledgement\nThese lecture notes for STAT 3255/5255 in Spring 2026 will be built upon the notes from Professor Jun Yan and former students enrolled in the course.\nFor those interested, class notes from Fall 2025, Spring 2025, Fall 2024, Spring 2024, Spring 2023, and Spring 2022 are publicly accessible. These archives offer insights into the evolution of the course content and the different perspectives brought by successive student cohorts.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Introduction to Data Science",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nWe will adopt a cooperative approach, facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/ids-s26.\nStudents will be asked to contribute to the notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#compiling-the-classnotes",
    "href": "index.html#compiling-the-classnotes",
    "title": "Introduction to Data Science",
    "section": "Compiling the Classnotes",
    "text": "Compiling the Classnotes\nTo reproduce the classnotes output on your own computer, here are the necessary steps. See Section Compiling the Classnotes for details.\n\nClone the classnotes repository to an appropriate location on your computer; see Chapter 2  Project Management for using Git.\nSet up a Python virtual environment in the root folder of the source; see Section Virtual Environment.\nActivate your virtual environment.\nInstall all the packages specified in requirements.txt in your virtual environment:\n\npip install -r requirements.txt\n\nFor some chapters that need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named api_key.txt in the root folder of the source.\nRender the book with quarto render from the root folder on a terminal; the rendered book will be stored under _book.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#midterm-exam",
    "href": "index.html#midterm-exam",
    "title": "Introduction to Data Science",
    "section": "Midterm Exam",
    "text": "Midterm Exam\nTBD",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project",
    "href": "index.html#final-project",
    "title": "Introduction to Data Science",
    "section": "Final Project",
    "text": "Final Project\nStudents are encouraged to start designing their final projects from the beginning of the semester. There are many open data that can be used. Here is a list of useful data challenges:\n\nASA Data Challenge Expo: What works in education?\nKaggle.\nDrivenData.\n15 Data Science Hackathons to Test Your Skills\nopenFDA\nIf you work on sports analytics, you are welcome to submit a poster to Connecticut Sports Analytics Symposium (CSAS) 2026. A good resource for sports analytics is ScoreNetwork.\nPaleobiology Database.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Introduction to Data Science",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#wishlist",
    "href": "index.html#wishlist",
    "title": "Introduction to Data Science",
    "section": "Wishlist",
    "text": "Wishlist\nThis is a wish list from all members of the class (alphabetical order, last name first, comma, then first name). Here is an example.\n\nChen, Kun\n\nIntroduce practical data science tools to undergraduates.\nPass real-world data science project experience to students.\nTeach student to think critically and statistically.\n\n\nAdd yours through a pull request; note the syntax of nested list in Markdown.\n\nStudents in STAT 3255\n\nLast name, First name\n\nWish 1\nWish 2\nWish 3\n\nBennett, Emily\n\nBecome more experienced and confident with Python\nGain experience with data science problems and relate them to the real world\nDevelop better understanding in GitHub, Git, and VSCode\n\nBudnick, Kayleigh\nBurns, Kyle\nCarbone, Vincenzo\n\nConnect data science to the world of sports analytics.\nImprove my literacy with VS code, quarto, and git.\nHave an in depth understanding of python code.\n\nDavis, Reid\n\nFocus on real world data and modeling to solve problems\nWork start to finish on a full fledged data science project\nImprove my knowledge of Python, Git, Quarto and working in repositories\n\nDesai, Alysha\n\nGain more hands-on experience cleaning, organizing, and breaking apart real-world datasets to better understand how raw data is transformed into usable insights.\nImprove my skills in preparing presentations of my findings and become more confident explaining more complex topics to an audience.\nBecome more familiar with version control tools like Git and GitHub\n\nFaisal, Zaynab\n\nGain a better understanding of data science and real life applications\nBecome more comfortable using Git/GitHub\nImprove python and coding skills\n\nIbrahim, Omar\nJackson, Brooke\nJiang, Ryan\n\nEnhance my Python and coding abilities\nIntroduce myself to the world of Data Science\nBecoming more comfortable with being able to present findings on specific data and explaining findings more than just the words on the slides.\n\nJones, Cody\nKwak, Jinha\nLacasse, Violet\n\nLearn how to find/grab/handle datasets.\nBecome familiar with Git and how it can be used to collaborate.\nLearn more about data science and how it can help in my data analysis (domain: earth data science) major.\n\nLandolphi, Joseph\nLawrence, Claire\nLiu, Kevin\n\nUse git better in a collobrative environment and more complex commands\nApply data analysis to translate data to real-world results\nUnderstand the use of Quarto in data analysis\n\nMccabe, Scott\nMohan, Harish\nNash, Jayden\nOrsini, Ronnie\n\nLearn how to apply data science techniques to real world datasets\nImprove collaboration skills using Git and GitHub\nBetter understand Quarto for data analysis\n\nPatel, Reesha\n\nLearn about the process between identifying a problem/question and using data science to find solutitons/get insights.\nImprove my knowledge of packages in Python (scikit-learn, seaborn).\nApply what I’ve learned in my classes to actual, real-world datasets.\n\nPatel, Vrajkumar\nSawyer, Riley\n\nLearn how to process and interpret real data using Python.\nWork in a collaborative setting using GitHub.\nGain experience using Quarto and GitHub.\n\nSudarsanam, Shreya\n\nImprove my understanding of how to choose which programming language to use (i.e. R, Python, and SQL), and how to evaluate the strengths and weaknesses of popular languages in each step of the data analysis pipeline.\nBecome more familiar with machine learning in research and industry based contexts.\nLearn/Demystify the process of completing a data science project from start to finish. This may involve mining, cleaning, and visualizing data and creating machine learning models to help make valuable inferences.\n\nTessman, Sean\n\nLearn how data science applies to sports analytics and performance.\nBuild a strong reproducible workflow with Git, Quarto, and Python.\nGain experience with real-world datasets.\n\nTrnka, Jonathan\n\nI want to get more comfortable using github, linux commands, and more.\nLearn more about ML and AI modeling.\nI want to apply what I know to real world data sets from beginning to end.\n\nWatanabe, Sara\n\nLearn how to effectively use GitHub for collaboration on data\nLearn how to apply data science techniques to real world problems\nImprove my Python programming skills for data analysis\n\nWolven, Alexander\n\nBuild hirable skills in Python, Quarto, and Git through projects.\nLearn about machine learning and AI modeling.\nGain experience working with real datasets from cleaning to analysis and clear presentation.\n\nZhang, Jianan\n\nLearn how to use Git and GitHub effectively for collaborative data science projects.\nImprove Python data analysis skills (NumPy, Pandas, Matplotlib) through applied projects.\n\nZharyy, Sofia\n\n\n\nStudents in STAT 5255\n\nLast name, First name\n\nWish 1\nWish 2\nWish 3",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-logistics",
    "href": "index.html#course-logistics",
    "title": "Introduction to Data Science",
    "section": "Course Logistics",
    "text": "Course Logistics\n\nTopic Presentation Orders\nThe topic presentation order is set up in class.\n\nwith open('rosters/3255.txt', 'r') as file:\n    ug = [line.strip() for line in file]\nwith open('rosters/5255.txt', 'r') as file:\n    gr = [line.strip() for line in file]\n## presenters = ug + gr\npresenters = [x for x in (ug + gr) if x]   # removes empty lines\nprint(f\"Number of presenters: {len(presenters)}\")\n\nimport random\n## seed jointly set by the class\nseed_s26 = 723 + 2026 + 125 \nprint(f\"Random seed: {seed_s26}\")\nrandom.seed(seed_s26)\nrandom.sample(presenters, len(presenters))\n## random.shuffle(presenters) # This would shuffle the list in place\n\nNumber of presenters: 30\nRandom seed: 2874\n\n\n['Desai, Alysha',\n 'Kwak, Jinha',\n 'Jackson, Brooke',\n 'Sudarsanam, Shreya',\n 'Sawyer, Riley',\n 'Faisal, Zaynab',\n 'Tessman, Sean',\n 'Nash, Jayden',\n 'Watanabe, Sara',\n 'Jiang, Ryan',\n 'Jones, Cody',\n 'Davis, Reid',\n 'Lawrence, Claire',\n 'Trnka, Jonathan',\n 'Orsini, Ronnie',\n 'Patel, Vrajkumar',\n 'Liu, Kevin',\n 'Landolphi, Joseph',\n 'Burns, Kyle',\n 'Mohan, Harish',\n 'Carbone, Vincenzo',\n 'Bennett, Emily',\n 'Budnick, Kayleigh',\n 'Ibrahim, Omar',\n 'Patel, Reesha',\n 'Zhang, Jianan',\n 'Mccabe, Scott',\n 'Lacasse, Violet',\n 'Wolven, Alexander',\n 'Zharyy, Sofia']\n\n\nSwitching slots is allowed as long as you find someone who is willing to switch with you. In this case, make a pull request to switch the order and let me know.\nYou are welcome to choose a topic that you are interested the most, subject to some order restrictions. For example, decision tree should be presented before random forest or extreme gradient boosting. This justifies certain requests for switching slots.\n\n\nPresentation Task Board\nTalk to the professor about your topics at least one week prior to your scheduled presentation. Here are some example tasks:\n\nMarkdown jumpstart \nImport/Export data\nData manipulation with Pandas\nAccessing US census data \nDatabase operation with Structured Query Language (SQL)\nGrammar of graphics\nVisualizing spatial data\nSpatial data with GeoPandas\nVisualize spatial data in a Google map with gmplot\nAnimation\nStatistical analysis for proportions and rates\nFalse discovery rate control\nPrincipal component analysis\nMulti-dimensional scaling\nt-SNE\nUniform manifold approximation and projection (UMAP)\nAutoencoders\nK-means clustering\nFinite mixture model\nLeast absolute shrinkage and selection operator (Lasso)\nLogistic regression and its extensions\nSupport vector machine\nRandom forest\nGradient boosting machine \nNeural networks basics\nMLP/ANN/CNN/RNN/LSTM\nDeep learning\nNatural leanguage processing\nLarge language models (LLM)\nLLM agents \nAutomatic differentiation\nReinforcement learning\nDeveloping a Python package\nWeb scraping \n\n\n\nTopic Presentation Schedule\nThe topic presentation is 20 points. It includes:\n\nTopic selection consultation in advance (4 points).\nDelivering the presentation in class (8 points). Your presentation should be about 20 minutes.\nContribute to the class notes within two weeks following the presentation (8 points).\n\nPlease use the following table to sign up.\n\n\n\n\n\n\n\n\nDate\nPresenter\nTopic\n\n\n\n\n02/16\nDesai, Alysha\n\n\n\n02/16\nKwak, Jinha\n\n\n\n02/18\nJackson, Brooke\n\n\n\n02/18\nSudarsanam, Shreya\nDatabase operation with Structured Query Language (SQL)\n\n\n02/23\nSawyer, Riley\n\n\n\n02/23\nFaisal, Zaynab\nImport/Export Data\n\n\n02/25\nTessman, Sean\n\n\n\n02/25\nNash, Jayden\n\n\n\n03/02\nWatanabe, Sara\nRandom Forest\n\n\n03/02\nJiang, Ryan\n\n\n\n03/04\nJones, Cody\n\n\n\n03/04\nDavis, Reid\n\n\n\n03/09\nLawrence, Claire\n\n\n\n03/09\nTrnka, Jonathan\n\n\n\n03/11\nOrsini, Ronnie\n\n\n\n03/11\nPatel, Vrajkumar\n\n\n\n03/23\nLiu, Kevin\n\n\n\n03/23\nLandolphi, Joseph\n\n\n\n03/25\nBurns, Kyle\n\n\n\n03/25\nMohan, Harish\n\n\n\n03/30\nCarbone, Vincenzo\n\n\n\n03/30\nBennett, Emily\n\n\n\n04/01\nBudnick, Kayleigh\n\n\n\n04/01\nIbrahim, Omar\n\n\n\n04/06\nPatel, Reesha\n\n\n\n04/06\nZhang, Jianan\n\n\n\n04/08\nMccabe, Scott\n\n\n\n04/08\nLacasse, Violet\n\n\n\n04/13\nWolven, Alexander\n\n\n\n04/13\nZharyy, Sofia\n\n\n\n\n\n\nFinal Project Presentation Schedule\nWe use the same order as the topic presentation for undergraduate final presentation. An introduction on how to use Quarto to prepare presentation slides is available under the templates directory in the classnotes source tree, thank to Zachary Blanchard, which can be used as a template to start with.\n\n\n\nDate\nPresenter\n\n\n\n\n04/15\n\n\n\n04/20\n\n\n\n04/22\n\n\n\n04/27\n\n\n\n04/29\n\n\n\nScheduled Final Exam Time\n\n\n\n\n\n\nContributing to the Class Notes\nContribution to the class notes is through a `pull request’.\n\nSynchronize your local repo of the classnotes with my classnotes repo.\nStart a new branch and switch to the new branch.\nOn the new branch, add a qmd file for your presentation\nIf using Python, create and activate a virtual environment with requirements.txt\nWork on your qmd file, test with quarto render.\nWhen satisfied, commit and make a pull request with your quarto files and an updated requirements.txt.\n\nI have added a template file _mysection.qmd as an example, which is includeed in index.qmd. See also how _ethics.qmd is included into 05-ethics_communication.qmd for example.\nHere is a checklist to help smooth the process.\n\nGet approval for your topic at least one week in advance. Otherwise you loose points.\nNo plagiarism. Under no circumstances should you copy someone else’s notes and use it for your contribution.\nNo yaml header. The whole souce tree is controlled by _quarto.yml.\nThe top heading level of your contribuion is section (##). See existing sections for examples.\nKeep line width under 80 characters.\nInclude a subsection (###) on further readings.\nAvoide dependence on external files (e.g., data, images, etc.). Using example datasets that are already in the data folder or that come with Python packages.\nNo usage of copyrighted images.\nWhen citing article/book references, use BibTeX (learn how from our sources).\nTest on your own computer before making a pull request.\nSend me your presentation two days in advance if you want feedbacks.\n\nFor more detailed style guidance, please see notes on statistical writing.\nPlagiarism is to be prevented. Remember that these class notes are publicly available online with your names attached. Here are some resources on how to avoid plagiarism.\n\n\n\nHomework Logistics\n\nWorkflow of Submitting Homework Assisngment\n\nClick the GitHub classroom assignment link in HuskCT announcement.\nAccept the assignment and follow the instructions to an empty repository.\nMake a clone of the repo at an appropriate folder on your own computer with git clone.\nGo to this folder, add your qmd source, work on it, and group your changes to different commits.\nPush your work to your GitHub repo with git push.\nCreate a new release and put the generated pdf file in it for ease of grading.\n\n\n\nRequirements\n\nUse the repo from Git Classroom to submit your work. See Chapter 2  Project Management.\n\nKeep the repo clean (no tracking generated files).\n\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\n\nMake at least 10 commits and form a style of frequent small commits.\n\nTrack quarto sources only in your repo. See Chapter 3  Reproducible Data Science.\nFor the convenience of grading, add your standalone html or pdf output to a release in your repo.\nFor standalone pdf output, you will need to have LaTeX installed.\n\n\n\n\nQuizzes about Syllabus\n\nDo I accept late homework?\nCould you list a few examples of email etiquette?\nHow would you lose style points?\nWould you use CLI and GUI?  \nWhat’s the first date on which you have to complete something about your final project?\nCan you use AI for any task in this course?\nIf you need a reference letter, how could you help me to help you?",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#practical-tips",
    "href": "index.html#practical-tips",
    "title": "Introduction to Data Science",
    "section": "Practical Tips",
    "text": "Practical Tips\n\nData analysis\n\nUse an IDE so you can play with the data interactively\nCollect codes that have tested out into a script for batch processing\nDuring data cleaning, keep in mind how each variable will be used later\nNo keeping large data files in a repo; assume a reasonable location with your collaborators\n\n\n\nPresentation\n\nDon’t forget to introduce yourself if there is no moderator.\nHighlight your research questions and results, not code.\nGive an outline, carry it out, and summarize.\nUse your own examples to reduce the risk of plagiarism.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#my-presentation-topic-template",
    "href": "index.html#my-presentation-topic-template",
    "title": "Introduction to Data Science",
    "section": "My Presentation Topic (Template)",
    "text": "My Presentation Topic (Template)\nThis section was prepared by John Smith.\nUse Markdown syntax. If not clear on what to do, learn from the class notes sources.\n\nPay attention to the sectioning levels.\nCite references with their bib key.\nIn examples, maximize usage of data set that the class is familiar with.\nCould use datasets in Python packages or downloadable on the fly.\nTest your section by quarto render &lt;filename.qmd&gt;.\n\n\nIntroduction\nHere is an overview.\n\n\nSub Topic 1\nPut materials on topic 1 here\nPython examples can be put into python code chunks:\n\n# import pandas as pd\n\n# do something\n\n\n\nSub Topic 2\nPut materials on topic 2 here.\n\n\nSub Topic 3\nPut matreials on topic 3 here.\n\n\nConclusion\nPut sumaries here.\n\n\nFurther Readings\nPut links to further materials.\n\n\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data Science?\nData science is a multifaceted field, often conceptualized as resting on three fundamental pillars: mathematics/statistics, computer science, and domain-specific knowledge. This framework helps to underscore the interdisciplinary nature of data science, where expertise in one area is often complemented by foundational knowledge in the others.\nA compelling definition was offered by Prof. Bin Yu in her 2014 Presidential Address to the Institute of Mathematical Statistics. She defines \\[\\begin{equation*}\n\\mbox{Data Science} =\n\\mbox{S}\\mbox{D}\\mbox{C}^3,\n\\end{equation*}\\] where\nComputing underscores the need for proficiency in programming and algorithmic thinking, collaboration/teamwork reflects the inherently collaborative nature of data science projects, often requiring teams with diverse skill sets, and communication to outsiders emphasizes the importance of translating complex data insights into understandable and actionable information for non-experts.\nThis definition neatly captures the essence of data science, emphasizing a balance between technical skills, teamwork, and the ability to communicate effectively.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#what-is-data-science",
    "href": "01-intro.html#what-is-data-science",
    "title": "1  Introduction",
    "section": "",
    "text": "‘S’ represents Statistics, signifying the crucial role of statistical methods in understanding and interpreting data;\n‘D’ stands for domain or science knowledge, indicating the importance of specialized expertise in a particular field of study;\nthe three ’C’s denote computing, collaboration/teamwork, and communication to outsiders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#expectations-from-this-course",
    "href": "01-intro.html#expectations-from-this-course",
    "title": "1  Introduction",
    "section": "1.2 Expectations from This Course",
    "text": "1.2 Expectations from This Course\nIn this course, students will be expected to achieve the following outcomes:\n\nProficiency in Project Management with Git: Develop a solid understanding of Git for efficient and effective project management. This involves mastering version control, branching, and collaboration through this powerful tool.\nProficiency in Project Reporting with Quarto: Gain expertise in using Quarto for professional-grade project reporting. This encompasses creating comprehensive and visually appealing reports that effectively communicate your findings.\nHands-On Experience with Real-World Data Science Projects: Engage in practical data science projects that reflect real-world scenarios. This hands-on approach is designed to provide you with direct experience in tackling actual data science challenges.\nCompetency in Using Python and Its Extensions for Data Science: Build strong skills in Python, focusing on its extensions relevant to data science. This includes libraries like Pandas, NumPy, and Matplotlib, among others, which are critical for data analysis and visualization.\nFull Grasp of the Meaning of Results from Data Science Algorithms: Learn to not only apply data science algorithms but also to deeply understand the implications and meanings of their results. This is crucial for making informed decisions based on these outcomes.\nBasic Understanding of the Principles of Data Science Methods: Acquire a foundational knowledge of the underlying principles of various data science methods. This understanding is key to effectively applying these methods in practice.\nCommitment to the Ethics of Data Science: Emphasize the importance of ethical considerations in data science. This includes understanding data privacy, bias in data and algorithms, and the broader social implications of data science work.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#computing-environment",
    "href": "01-intro.html#computing-environment",
    "title": "1  Introduction",
    "section": "1.3 Computing Environment",
    "text": "1.3 Computing Environment\nAll setups are operating system dependent. As soon as possible, stay away from Windows. Otherwise, good luck (you will need it).\n\n1.3.1 Operating System\nYour computer has an operating system (OS), which is responsible for managing the software packages on your computer. Each operating system has its own package management system. For example:\n\nLinux: Linux distributions have a variety of package managers depending on the distribution. For instance, Ubuntu uses APT (Advanced Package Tool), Fedora uses DNF (Dandified Yum), and Arch Linux uses Pacman. These package managers are integral to the Linux experience, allowing users to install, update, and manage software packages easily from repositories.\nmacOS: macOS uses Homebrew as its primary package manager. Homebrew simplifies the installation of software and tools that aren’t included in the standard macOS installation, using simple commands in the terminal.\nWindows: Windows users often rely on the Microsoft Store for apps and software. For more developer-focused package management, tools like Chocolatey and Windows Package Manager (Winget) are used. Additionally, recent versions of Windows have introduced the Windows Subsystem for Linux (WSL). WSL allows Windows users to run a Linux environment directly on Windows, unifying Windows and Linux applications and tools. This is particularly useful for developers and data scientists who need to run Linux-specific software or scripts. It saves a lot of trouble Windows users used to have that users previously faced before WSL was introduced.\n\nUnderstanding the package management system of your operating system is crucial for effectively managing and installing software, especially for data science tools and applications.\n\n\n1.3.2 File System\nA file system is a fundamental aspect of a computer’s operating system, responsible for managing how data is stored and retrieved on a storage device, such as a hard drive, SSD, or USB flash drive. Essentially, it provides a way for the OS and users to organize and keep track of files. Different operating systems typically use different file systems. For instance, NTFS and FAT32 are common in Windows, APFS and HFS+ in macOS, and Ext4 in many Linux distributions. Each file system has its own set of rules for controlling the allocation of space on the drive and the naming, storage, and access of files, which impacts performance, security, and compatibility. Understanding file systems is crucial for tasks such as data recovery, disk partitioning, and managing file permissions, making it an important concept for anyone working with computers, especially in data science and IT fields.\nNavigating through folders in the command line, especially in Unix-like environments such as Linux or macOS, and Windows Subsystem for Linux (WSL), is an essential skill for effective file management. The command cd (change directory) is central to this process. To move into a specific directory, you use cd followed by the directory name, like cd Documents. To go up one level in the directory hierarchy, you use cd ... To return to the home directory, simply typing cd or cd ~ will suffice. The ls command lists all files and folders in the current directory, providing a clear view of your options for navigation. Mastering these commands, along with others like pwd (print working directory), which displays your current directory, equips you with the basics of moving around the file system in the command line, an indispensable skill for a wide range of computing tasks in Unix-like systems.\n\n\n1.3.3 Command Line Interface\nOn Linux or MacOS, simply open a terminal.\nOn Windows, several options can be considered.\n\nWindows Subsystem Linux (WSL): https://learn.microsoft.com/en-us/windows/wsl/\nCygwin (with X): https://x.cygwin.com\nGit Bash: https://www.gitkraken.com/blog/what-is-git-bash\n\nTo jump-start, here is a tutorial: Ubuntu Linux for beginners.\nAt least, you need to know how to handle files and traverse across directories. The tab completion and introspection supports are very useful.\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.3.4 Python\nSet up Python on your computer:\n\nPython 3.\nPython package manager miniconda or pip.\nIntegrated Development Environment (IDE) (Jupyter Notebook; RStudio; VS Code; Emacs; etc.)\n\nI will be using VS Code in class.\nReadability is important! Check your Python coding style against the recommended styles: https://peps.python.org/pep-0008/. A good place to start is the Section on “Code Lay-out”.\nOnline books on Python for data science:\n\n“Python Data Science Handbook: Essential Tools for Working with Data,” First Edition, by Jake VanderPlas, O’Reilly Media, 2016.\n“Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython.” Third Edition, by Wes McKinney, O’Reilly Media, 2022.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-git.html",
    "href": "02-git.html",
    "title": "2  Project Management",
    "section": "",
    "text": "2.1 Set Up Git/GitHub\nMany tutorials are available in different formats. Here is a YouTube video ``Git and GitHub for Beginners — Crash Course’’. The video also covers GitHub, a cloud service for Git which provides a cloud back up of your work and makes collaboration with co-workers easy. Similar services are, for example, bitbucket and GitLab.\nThere are tools that make learning Git easy.\nDownload Git if you don’t have it already.\nTo set up GitHub (other services like Bitbucket or GitLab are similar), you need to\nSee how to get started with GitHub account.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#set-up-gitgithub",
    "href": "02-git.html#set-up-gitgithub",
    "title": "2  Project Management",
    "section": "",
    "text": "Generate an SSH key if you don’t have one already.\nSign up an GitHub account.\nAdd the SSH key to your GitHub account",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#most-frequently-used-git-commands",
    "href": "02-git.html#most-frequently-used-git-commands",
    "title": "2  Project Management",
    "section": "2.2 Most Frequently Used Git Commands",
    "text": "2.2 Most Frequently Used Git Commands\nThe following seven commands will get you started and they may be all that you need most of the time.\n\ngit clone:\n\nUsed to clone a repository to a local folder.\nRequires either HTTPS link or SSH key to authenticate.\n\ngit pull:\n\nDownloads any updates made to the remote repository and automatically updates the local repository.\n\ngit status:\n\nReturns the state of the working directory.\nLists the files that have been modified, and are yet to be or have been staged and/or committed.\nShows if the local repository is begind or ahead a remote branch.\n\ngit add:\n\nAdds new or modified files to the Git staging area.\nGives the option to select which files are to be sent to the remote repository\n\ngit rm:\n\nUsed to remove files from the staging index or the local repository.\n\ngit commit:\n\nCommits changes made to the local repository and saves it like a snapshot.\nA message is recommended with every commit to keep track of changes made.\n\ngit push:\n\nUsed to send commits made on local repository to the remote repository.\n\n\nFor more advanced usages:\n\ngit diff\ngit branch\ngit reset",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#tips-on-using-git",
    "href": "02-git.html#tips-on-using-git",
    "title": "2  Project Management",
    "section": "2.3 Tips on using Git:",
    "text": "2.3 Tips on using Git:\n\nUse the command line interface instead of the web interface (e.g., upload on GitHub)\nMake frequent small commits instead of rare large commits.\nMake commit messages informative and meaningful.\nName your files/folders by some reasonable convention.\n\nLower cases are better than upper cases.\nNo blanks in file/folder names.\n\nKeep the repo clean by not tracking generated files.\nCreat a .gitignore file for better output from git status.\nKeep the linewidth of sources to under 80 for better git diff view.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "02-git.html#pull-request",
    "href": "02-git.html#pull-request",
    "title": "2  Project Management",
    "section": "2.4 Pull Request",
    "text": "2.4 Pull Request\nTo contribute to an open source project (e.g., our classnotes), use pull requests. Pull requests “let you tell others about changes you’ve pushed to a branch in a repository on GitHub. Once a pull request is opened, you can discuss and review the potential changes with collaborators and add follow-up commits before your changes are merged into the base branch.”\nWatch this YouTube video: GitHub pull requests in 100 seconds.\nThe following are step-by-step instructions on how to make a pull request to the class notes contributed by Nick Pfeifer..\n\nCreate a fork of the class repository on the GitHub website.\n\nMake sure your fork is up to date by clicking Sync fork if necessary.\n\nClone your fork into a folder on your computer.\n\ngit clone https://github.com/GitHub_Username/ids-s26.git\nReplace GitHub_Username with your personal GitHub Username.\n\nCheck to see if you can access the folder/cloned repository in your code editor.\n\nThe class notes home page is located in the index.qmd file.\n\nMake a branch and give it a good name.\n\nMove into the directory with the cloned repository.\nCreate a branch using:\n\ngit checkout -b branch_name\nReplace branch_name with a more descriptive name.\n\nYou can check your branches using:\n\ngit branch\nThe branch in use will have an asterisk to the left of it.\n\nIf you are not in the right branch you can use the following command:\n\ngit checkout existing-branch\nReplace existing-branch with the name of the branch you want to use.\n\n\nRun git status to verify that no changes have been made.\nMake changes to a file in the class notes repository.\n\nFor example: add your wishes to the Wishlist in index.qmd using nested list syntax in markdown.\nRemember to save your changes.\n\nRun git status again to see that changes have been made.\nUse the add command.\n\ngit add filename\nExample usage: git add index.qmd\n\nMake a commit.\n\ngit commit -m \"Informative Message\"\nBe clear about what you changed and perhaps include your name in the message.\n\nPush the files to GitHub.\n\ngit push origin branch-name\nReplace branch-name with the name of your current branch.\n\nGo to your forked repository on GitHub and refresh the page, you should see a button that says Compare and Pull Request.\n\nDescribe the changes you made in the pull request.\nClick Create pull request.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management</span>"
    ]
  },
  {
    "objectID": "03-quarto.html",
    "href": "03-quarto.html",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "3.1 Introduction to Quarto\nData science projects should be reproducible to be trustworthy. Dynamic documents facilitate reproducibility. Quarto is an open-source dynamic document preparation system, ideal for scientific and technical publishing. From the official websites, Quarto can be used to:\nTo get started with Quarto, see documentation at Quarto.\nFor a clean style, I suggest that you use VS Code as your IDE. The ipynb files have extra formats in plain texts, which are not as clean as qmd files. There are, of course, tools to convert between the two representations of a notebook. For example:\nWe will use Quarto for homework assignments, classnotes, and presentations. You will see them in action through in-class demonstrations. The following sections in the Quarto Guide are immediately useful.\nA template for homework is in this repo (hwtemp.qmd) to get you started with homework assignments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#introduction-to-quarto",
    "href": "03-quarto.html#introduction-to-quarto",
    "title": "3  Reproducible Data Science",
    "section": "",
    "text": "quarto convert hello.ipynb # converts to qmd\nquarto convert hello.qmd   # converts to ipynb\n\n\nMarkdown basics\nUsing Python\nPresentations",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#sec-buildnotes",
    "href": "03-quarto.html#sec-buildnotes",
    "title": "3  Reproducible Data Science",
    "section": "3.2 Compiling the Classnotes",
    "text": "3.2 Compiling the Classnotes\nThe sources of the classnotes are at https://github.com/statds/ids-s26. This is also the source tree that you will contributed to this semester. I expect that you clone the repository to your own computer, update it frequently, and compile the latest version on your computer (reproducibility).\nTo compile the classnotes, you need the following tools: Git, Quarto, and Python.\n\n3.2.1 Clone the Repository\nClone the repository to your own computer. In a terminal (command line), go to an appropriate directory (folder), and clone the repo. For example, if you use ssh for authentication:\ngit clone git@github.com:statds/ids-s26.git\n\n\n3.2.2 Set up your Python Virtual Environment\nFor reproducibility, the book uses two Python virtual environments. A virtual environment is a directory containing a self-contained Python interpreter and the software packages needed for a project. Using virtual environments isolates the dependencies for these classnotes from both the system installation and other projects. This happens in the cloned project folder.\ncd ids-s26\nThe default environment supports all chapters.\n\n\n3.2.2.1 Default Environment\nCreate the default environment in the current directory (the Python version used for this was 3.11):\npython3.11 -m venv .ids-s26\nActivate it:\n. .ids-s26/bin/activate\nInstall the required packages:\npip install -r requirements.txt\nWhen activated, your shell prompt begins with (.ids-s26), and Python uses the packages installed in this environment. To exit the environment:\ndeactivate\n\n\n\n\n3.2.3 Render the Classnotes\nAssuming quarto has been set up, we render the classnotes in the cloned repository.\nMost chapters are to be rendered under the .ids-s26 virtual environment:\n. .ids-s26/bin/activate\nquarto render\ndeactivate\n\nIf there are error messages, search and find solutions to clear them. Otherwise, the html version of the notes will be available under _book/index.html, which is default location of the output.\n\n\n3.2.4 Login Requirements\nFor some illustrations, you need to interact with certain sites that require account information. For example, for Google map services, you need to save your API key in a file named gmKey.txt in the root folder of the source. Another example is to access the US Census API, where you would need to register an account and get your Census API Key.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "03-quarto.html#the-data-science-life-cycle",
    "href": "03-quarto.html#the-data-science-life-cycle",
    "title": "3  Reproducible Data Science",
    "section": "3.3 The Data Science Life Cycle",
    "text": "3.3 The Data Science Life Cycle\nThis section summarizes Chapter 2 of Veridical Data Science (Yu & Barter, 2024), which introduces the data science life cycle (DSLC). The DSLC provides a structured way to think about the progression of data science projects. It consists of six stages, each with a distinct purpose:\n\nStage 1: Problem formulation and data collection\nCollaborate with domain experts to refine vague questions into ones that can realistically be answered with data. Identify what data already exists or design new collection protocols. Understanding the collection process is crucial for assessing how data relates to reality.\nStage 2: Data cleaning, preprocessing, and exploratory data analysis\nClean data to make it tidy, unambiguous, and correctly formatted. Preprocess it to meet the requirements of specific algorithms, such as handling missing values or scaling variables. Exploratory data analysis (EDA) summarizes patterns using tables, statistics, and plots, while explanatory data analysis polishes visuals for communication.\nStage 3: Exploring intrinsic data structures (optional)\nTechniques such as dimensionality reduction simplify data into lower-dimensional forms, while clustering identifies natural groupings among observations. Even if not central to the project, these methods often enhance understanding.\nStage 4: Predictive and/or inferential analysis (optional)\nMany projects are cast as prediction tasks, training algorithms like regression or random forests to forecast outcomes. Inference focuses on estimating population parameters and quantifying uncertainty. This book emphasizes prediction while acknowledging inference as important in many domains.\nStage 5: Evaluation of results\nFindings should be evaluated both qualitatively, through critical thinking, and quantitatively, through the PCS framework. PCS stands for predictability, computability, and stability:\n\nPredictability asks whether findings hold up in relevant future data.\n\nComputability asks whether methods are feasible with available computational resources.\n\nStability asks whether conclusions remain consistent under reasonable changes in data, methods, or judgment calls.\nTogether, PCS provides a foundation for assessing the reliability of data-driven results.\n\nStage 6: Communication of results\nResults must be conveyed clearly to intended audiences, whether through reports, presentations, visualizations, or deployable tools. Communication should be tailored so findings can inform real-world decisions.\n\nThe DSLC is not a linear pipeline—analysts often loop back to refine earlier steps. The chapter also cautions against data snooping, where patterns discovered during exploration are mistaken for reliable truths. Applying PCS ensures that results are not only technically sound but also trustworthy and interpretable across the life cycle.\n\n\n\n\n\n\nYu, B., & Barter, R. L. (2024). Veridical data science: The practice of responsible data analysis and decision making. MIT Press.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Reproducible Data Science</span>"
    ]
  },
  {
    "objectID": "04-python.html",
    "href": "04-python.html",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "4.1 The Python World\nYou have programmed in Python. Regardless of your skill level, let us do some refreshing.\nSee, for example, how to build a Python libratry.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#the-python-world",
    "href": "04-python.html#the-python-world",
    "title": "4  Python Refreshment",
    "section": "",
    "text": "Function: a block of organized, reusable code to complete certain task.\nModule: a file containing a collection of functions, variables, and statements.\nPackage: a structured directory containing collections of modules and an __init.py__ file by which the directory is interpreted as a package.\nLibrary: a collection of related functionality of codes. It is a reusable chunk of code that we can use by importing it in our program, we can just use it by importing that library and calling the method of that library with period(.).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#standard-library",
    "href": "04-python.html#standard-library",
    "title": "4  Python Refreshment",
    "section": "4.2 Standard Library",
    "text": "4.2 Standard Library\nPython’s has an extensive standard library that offers a wide range of facilities as indicated by the long table of contents listed below. See documentation online.\n\nThe library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs by abstracting away platform-specifics into platform-neutral APIs.\n\nQuestion: How to get the constant \\(e\\) to an arbitary precision?\nThe constant is only represented by a given double precision.\n\nimport math\nprint(\"%0.20f\" % math.e)\nprint(\"%0.80f\" % math.e)\n\n2.71828182845904509080\n2.71828182845904509079559829842764884233474731445312500000000000000000000000000000\n\n\nNow use package decimal to export with an arbitary precision.\n\nimport decimal  # for what?\n\n## set the required number digits to 150\ndecimal.getcontext().prec = 150\ndecimal.Decimal(1).exp().to_eng_string()\ndecimal.Decimal(1).exp().to_eng_string()[2:]\n\n#print(decimal.Decimal(1).exp())\n\n'71828182845904523536028747135266249775724709369995957496696762772407663035354759457138217852516642742746639193200305992181741359662904357290033429526'",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#important-libraries",
    "href": "04-python.html#important-libraries",
    "title": "4  Python Refreshment",
    "section": "4.3 Important Libraries",
    "text": "4.3 Important Libraries\n\nNumPy\n\nFast arrays and math\n\npandas\n\nData wrangling with labeled tables\n\nmatplotlib\n\nCore plotting library\n\nIPython/Jupyter\n\nInteractive computing environment\n\nSciPy\n\nScientific algorithms built on NumPy\n\nscikit-learn\n\nMachine learning toolkit\n\nstatsmodels\n\nClassical statistics/econometrics\n\n\nQuestion: how to draw a random sample from a normal distribution and evaluate the density and distributions at these points?\n\nfrom scipy.stats import norm\n\nmu, sigma = 2, 4\nmean, var, skew, kurt = norm.stats(mu, sigma, moments='mvsk')\nprint(mean, var, skew, kurt)\nx = norm.rvs(loc = mu, scale = sigma, size = 10)\nx\n\n# help(norm.stats)\n\n2.0 16.0 0.0 0.0\n\n\narray([ 3.08585353,  4.33296223,  6.31376474, -1.17971323, 10.58451188,\n        0.65901574, -1.97256961, -1.69086369,  1.09478822,  7.8881342 ])\n\n\nThe pdf and cdf can be evaluated:\n\nnorm.pdf(x, loc = mu, scale = sigma)\n\n# norm.cdf(x, loc = mu, scale = sigma)\n\narray([0.09612757, 0.08413626, 0.0557571 , 0.0727164 , 0.00997009,\n       0.0942855 , 0.06090751, 0.06515832, 0.09721411, 0.03375339])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#writing-a-function",
    "href": "04-python.html#writing-a-function",
    "title": "4  Python Refreshment",
    "section": "4.4 Writing a Function",
    "text": "4.4 Writing a Function\n\n4.4.1 Example: Fibonacci Sequence\nConsider the Fibonacci Sequence \\(1, 1, 2, 3, 5, 8, 13, 21, 34, ...\\). The next number is found by adding up the two numbers before it. We are going to use 3 ways to solve the problems.\nThe first is a recursive solution.\n\ndef fib_rs(n):\n    if (n==1 or n==2):\n        return 1\n    else:\n        return fib_rs(n - 1) + fib_rs(n - 2)\n\n%timeit fib_rs(10)\n\n# import timeit\n# t = timeit.timeit(lambda: fib_rs(10), number=10000)\n# print(t, \"seconds total\")\n# print(t/10000, \"seconds per call\")\n\n2.68 μs ± 20.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe second uses dynamic programming memoization.\n\ndef fib_dm_helper(n, mem):\n    if mem[n] is not None:\n        return mem[n]\n    elif (n == 1 or n == 2):\n        result = 1\n    else:\n        result = fib_dm_helper(n - 1, mem) + fib_dm_helper(n - 2, mem)\n    mem[n] = result\n    return result\n\ndef fib_dm(n):\n    mem = [None] * (n + 1)\n    return fib_dm_helper(n, mem)\n\n%timeit fib_dm(10)\n\n629 ns ± 6.46 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nThe third is still dynamic programming but bottom-up.\n\ndef fib_dbu(n):\n    mem = [None] * (n + 1)\n    mem[1] = 1;\n    mem[2] = 1;\n    for i in range(3, n + 1):\n        mem[i] = mem[i - 1] + mem[i - 2]\n    return mem[n]\n\n\n%timeit fib_dbu(500)\n\n17.2 μs ± 227 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nThe fourth method: If we only need fib(n) (not all previous values), we can reduce memory usage by storing only the last two values.\n\ndef fib_dbu_m(n):\n    if n &lt;= 2:\n        return 1\n    a, b = 1, 1\n    for _ in range(3, n + 1):\n        a, b = b, a + b\n    return b\n\n%timeit fib_dbu_m(500)    \n\n8.99 μs ± 141 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\nApparently, the solutions have very different performance for larger n. We could more rigrously analyze the computational complexity and memory usage of each implementation.\n\n\n4.4.2 Basic Coding Structures\nMost Python/R programs are built from three core control structures:\n\nSequence: run statements top-to-bottom\nBranching: choose a path using if / elif / else\nLooping: repeat work using for or while\n\nThese show up everywhere in data science coding for data cleaning, simulation, feature engineering, model evaluation, etc.\n\n4.4.2.1 Sequence structure\nIn a sequence, the program executes statements in order. Later lines can use variables created earlier.\n\n# A simple \"data pipeline\" in sequence: define -&gt; compute -&gt; report\nx = [2, 4, 4, 4, 5, 5, 7, 9]\n\nn = len(x)\nmean_x = sum(x) / n\nvar_x = sum((xi - mean_x) ** 2 for xi in x) / (n - 1)   # sample variance\nsd_x = var_x ** 0.5\n\nmean_x, sd_x\n\n(5.0, 2.138089935299395)\n\n\nA helpful habit: write code as a readable sequence of steps, with meaningful variable names.\n\n\n\n4.4.2.2 Branching structure\nBranching lets the program make decisions based on conditions.\n\ndef letter_grade(score):\n    if score &gt;= 93:\n        return \"A\"\n    elif score &gt;= 90:\n        return \"A-\"\n    elif score &gt;= 87:\n        return \"B+\"\n    elif score &gt;= 83:\n        return \"B\"\n    elif score &gt;= 80:\n        return \"B-\"\n    else:\n        return \"below B-\"\n\nfor s in [97, 91, 88, 81, 72]:\n    print(s, \"-&gt;\", letter_grade(s))\n\n97 -&gt; A\n91 -&gt; A-\n88 -&gt; B+\n81 -&gt; B-\n72 -&gt; below B-\n\n\nCommon patterns:\n\nRange checks with &lt;=, &lt;, &gt;=, &gt;\nLogic with and, or, not\nMembership tests with in\n\n\nx = 7\n\n(x &gt; 0) and (x &lt; 10), (x in [1, 3, 5, 7])\n\n(True, True)\n\n\n\n\n\n4.4.2.3 Looping structure\nLoops repeat a block of code.\n\n4.4.2.3.1 for loops (iterate over a collection)\n\nvalues = [3, 1, 4, 1, 5]\n\ntotal = 0\nfor v in values:\n    total += v\n\ntotal\n\n14\n\n\nOften you want both index and value:\n\nfor i, v in enumerate(values):\n    print(i, v)\n\n0 3\n1 1\n2 4\n3 1\n4 5\n\n\n\n\n4.4.2.3.2 while loops (repeat until a condition becomes false)\nUse while when you don’t know in advance how many steps you need.\n\n# Keep doubling until we reach 1000\nx = 3\nsteps = 0\n\nwhile x &lt; 1000:\n    x *= 2\n    steps += 1\n\nx, steps\n\n(1536, 9)\n\n\n\n\n4.4.2.3.3 break and continue\n\nbreak exits the loop early\ncontinue skips to the next iteration\n\n\nnums = [2, 7, -1, 5, 0, 9]\n\nfor n in nums:\n    if n &lt; 0:\n        print(\"Found a negative, stopping.\")\n        break\n    if n == 0:\n        continue\n    print(\"Reciprocal:\", 1 / n)\n\nReciprocal: 0.5\nReciprocal: 0.14285714285714285\nFound a negative, stopping.\n\n\n\n\n\n4.4.2.4 Avoid loops: list comprehensions and vectorized operations\nPythonic code often avoids manual loops for simple transformations:\n\nvalues = [3, 1, 4, 1, 5]\n\nsquares = [v**2 for v in values]\nbig_only = [v for v in values if v &gt;= 4]\n\nsquares, big_only\n\n([9, 1, 16, 1, 25], [4, 5])\n\n\n\nimport numpy as np\n\n# Square a large vector: Python loop vs NumPy vectorization\ndef square_loop(x_list):\n    out = []\n    for v in x_list:\n        out.append(v * v)\n    return out\n\ndef square_vectorized(x_np):\n    return x_np * x_np\n\nn = 2_000_000\nx_list = list(range(n))\nx_np = np.arange(n)\n\n# Timing \n%timeit square_loop(x_list)\n%timeit square_vectorized(x_np)\n\n36.1 ms ± 1.11 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n516 μs ± 11.5 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n# Filtering: Python loop vs NumPy boolean masking\ndef filter_loop(x_list, threshold):\n    out = []\n    for v in x_list:\n        if v &gt;= threshold:\n            out.append(v)\n    return out\n\ndef filter_vectorized(x_np, threshold):\n    return x_np[x_np &gt;= threshold]\n\nthreshold = n // 2\n\n%timeit filter_loop(x_list, threshold)\n%timeit filter_vectorized(x_np, threshold)\n\n18.4 ms ± 541 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n645 μs ± 5.02 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nIn real data science workflows, libraries like NumPy and pandas provide vectorized operations that are faster than Python-level loops. Still, loops are essential for simulations, custom logic, and “glue code”.\n\n\n\n4.4.2.5 Other structures\nBeyond sequence, branching, and looping, Python has a few other common “building blocks” that show up frequently in real programs and data workflows.\nException handling (try / except) lets your code respond to errors gracefully (e.g., messy input data) instead of crashing.\n\ndef safe_float(s):\n    try:\n        return float(s)\n    except ValueError:\n        return np.nan\n\n[safe_float(x) for x in [\"3.14\", \"oops\", \"2.0\"]]\n\n[3.14, nan, 2.0]\n\n\nIteration helpers make loops cleaner and less error-prone.\n\nnames = [\"Ada\", \"Linus\", \"Grace\"]\nscores = [98, 85, 92]\n\nfor i, (name, score) in enumerate(zip(names, scores), start=1):\n    print(i, name, score)\n\n1 Ada 98\n2 Linus 85\n3 Grace 92\n\n\nContext managers (with) manage resources automatically—especially files—so they get closed even if something goes wrong.\n\nwith open(\"example.txt\", \"w\") as f:\n    f.write(\"Hello!\\n\")\n\nwith open(\"example.txt\", \"r\") as f:\n    text = f.read()\n\ntext\n\n'Hello!\\n'\n\n\nObject-oriented structure (classes) groups data and behavior together. You’ll see this in libraries, and occasionally you’ll write simple classes yourself.\n\nclass RunningMean:\n    def __init__(self):\n        self.total = 0.0\n        self.n = 0\n\n    def update(self, x):\n        self.total += x\n        self.n += 1\n\n    def value(self):\n        return self.total / self.n\n\nrm = RunningMean()\nfor x in [2, 4, 6]:\n    rm.update(x)\n\nrm.value()\n\n4.0\n\n\n\n\n\n4.4.3 Example: Monty Hall\nHere is a function that performs the Monty Hall experiments. In this version, the host opens only one empty door.\n\nimport numpy as np\n\ndef montyhall(n_doors, n_trials):\n    doors = np.arange(1, n_doors + 1)\n    prize = np.random.choice(doors, size=n_trials)\n    player = np.random.choice(doors, size=n_trials)\n    host = np.array([np.random.choice([d for d in doors\n                                       if d not in [player[x], prize[x]]])\n                     for x in range(n_trials)])\n    player2 = np.array([np.random.choice([d for d in doors\n                                          if d not in [player[x], host[x]]])\n                        for x in range(n_trials)])\n    return {'noswitch': np.sum(prize == player),\n               'switch': np.sum(prize == player2)}\n\nTest it out with 3 doors.\n\nmontyhall(3, 1000)\n\n{'noswitch': 326, 'switch': 674}\n\n\nThen with 4 doors\n\nmontyhall(4, 1000)\n\n{'noswitch': 280, 'switch': 369}\n\n\nThe true value for the two strategies with \\(n\\) doors are, respectively, \\(1 / n\\) and \\(\\frac{n - 1}{n (n - 2)}\\).\n\n\n4.4.3.1 Faster version\nThis one avoid loops.\n\nimport numpy as np\nfrom typing import Dict, Optional\n\ndef montyhall_fast(\n    n_doors: int,\n    n_trials: int,\n    seed: Optional[int] = None\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Run a Monty Hall simulation with `n_doors` doors and `n_trials` repetitions.\n    The host always opens exactly one empty door that is neither the prize door\n    nor the player's initial choice.\n\n    Parameters\n    ----------\n    n_doors : int\n        Total number of doors in the game (must be &gt;= 3).\n    n_trials : int\n        Number of independent trials to simulate.\n    seed : int, optional\n        Random seed for reproducibility.\n\n    Returns\n    -------\n    Dict[str, int]\n        A dictionary with counts of wins under two strategies:\n        - 'noswitch': staying with the initial choice\n        - 'switch'  : switching after the host reveals one empty door\n\n    Examples\n    --------\n    &gt;&gt;&gt; results = montyhall_fast(3, 100000, seed=42)\n    &gt;&gt;&gt; results\n    {'noswitch': 33120, 'switch': 66880}\n    \"\"\"\n    # Create a random number generator\n    rng = np.random.default_rng(seed)\n\n    # Initial random assignments\n    # Generate prize location and player choice for all trials\n    prize = rng.integers(n_doors, size=n_trials, dtype=np.int32)\n    player = rng.integers(n_doors, size=n_trials, dtype=np.int32)\n\n    # Allocate array for the host's opened door\n    host = np.empty(n_trials, dtype=np.int32)\n\n    # Case 1: player == prize → host excludes only 'player'\n    # 'same' is a boolean array: same[i] = True if the player \n    # initially picked the prize in trial i.\n    same = prize == player\n    if np.any(same):\n        k = rng.integers(n_doors - 1, size=np.sum(same), dtype=np.int32)\n        p = player[same]\n        # If k is before p, keep it.\n        # If k is at/after p, shift it up by 1 to “jump over” p.\n        host[same] = k + (k &gt;= p)\n\n    # Case 2: player != prize → host excludes 'player' and 'prize'\n    diff = ~same\n    if np.any(diff):\n        a = np.minimum(player[diff], prize[diff])\n        b = np.maximum(player[diff], prize[diff])\n        k = rng.integers(n_doors - 2, size=np.sum(diff), dtype=np.int32)\n        host[diff] = k + (k &gt;= a) + (k &gt;= (b-1))\n\n    # Player switches: exclude 'player' and 'host'\n    a2 = np.minimum(player, host)\n    b2 = np.maximum(player, host)\n    k2 = rng.integers(n_doors - 2, size=n_trials, dtype=np.int32)\n    player2 = k2 + (k2 &gt;= a2) + (k2 &gt;= (b2-1))\n\n    return {\n        \"noswitch\": int((prize == player).sum()),\n        \"switch\": int((prize == player2).sum()),\n    }\n\nAnother faster version uses Numba just-in-time (JIT) compiler Python, which translates a subset of Python and Numpy into fast machine code via LLVM at runtime. A decorator is added to the numeric functions and, after a one-time compile on the first call (“warm-up”), later calls run much faster. One can toggle parallel=True for multi-core speedups.\n\nfrom typing import Dict, Optional\nimport numpy as np\nfrom numba import njit, prange\n\n# --- internal helpers (compiled) ---\n\n@njit\ndef _seed_numba(seed: int) -&gt; None:\n    # Seed the RNG used inside Numba-compiled code\n    np.random.seed(seed)\n\n@njit\ndef _trial_once(n_doors: int) -&gt; (int, int):\n    \"\"\"\n    Run one Monty Hall trial (host opens exactly one empty door).\n    Returns (noswitch_win, switch_win) as 0/1 ints.\n    \"\"\"\n    prize  = np.random.randint(0, n_doors)\n    player = np.random.randint(0, n_doors)\n\n    # host picks an empty door not equal to prize or player\n    if player == prize:\n        # exclude only 'player' (size n_doors-1)\n        k = np.random.randint(0, n_doors - 1)\n        host = k + (1 if k &gt;= player else 0)\n    else:\n        # exclude 'player' and 'prize' (size n_doors-2)\n        a = player if player &lt; prize else prize\n        b = prize if prize &gt; player else player\n        k = np.random.randint(0, n_doors - 2)\n        host = k + (1 if k &gt;= a else 0) + (1 if k &gt;= (b-1) else 0)\n\n    # player switches: choose uniformly from doors != player and != host\n    a2 = player if player &lt; host else host\n    b2 = host if host &gt; player else player\n    k2 = np.random.randint(0, n_doors - 2)\n    player2 = k2 + (1 if k2 &gt;= a2 else 0) + (1 if k2 &gt;= (b2-1) else 0)\n\n    noswitch_win = 1 if prize == player  else 0\n    switch_win   = 1 if prize == player2 else 0\n    return noswitch_win, switch_win\n\n@njit(parallel=True)\ndef _run_parallel(n_doors: int, n_trials: int) -&gt; (int, int):\n    ns = np.zeros(n_trials, dtype=np.int64)\n    sw = np.zeros(n_trials, dtype=np.int64)\n    for i in prange(n_trials):\n        a, b = _trial_once(n_doors)\n        ns[i] = a\n        sw[i] = b\n    return int(ns.sum()), int(sw.sum())\n\n@njit\ndef _run_serial(n_doors: int, n_trials: int) -&gt; (int, int):\n    noswitch = 0\n    switch   = 0\n    for _ in range(n_trials):\n        a, b = _trial_once(n_doors)\n        noswitch += a\n        switch   += b\n    return noswitch, switch\n\n# --- public API ---\n\ndef montyhall_numba(\n    n_doors: int,\n    n_trials: int,\n    seed: Optional[int] = None,\n    parallel: bool = True,\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Monty Hall (host opens one empty door) using Numba-compiled loops.\n\n    Parameters\n    ----------\n    n_doors : int\n        Number of doors (&gt;= 3).\n    n_trials : int\n        Number of trials to simulate.\n    seed : int, optional\n        Seed for reproducibility.\n    parallel : bool, default True\n        Use a parallel loop over trials (multi-core).\n\n    Returns\n    -------\n    Dict[str, int]\n        {'noswitch': ..., 'switch': ...}\n    \"\"\"\n    if n_doors &lt; 3:\n        raise ValueError(\"n_doors must be &gt;= 3\")\n\n    if seed is not None:\n        _seed_numba(int(seed))  # seed the compiled RNG\n\n    ns, sw = (_run_parallel(n_doors, n_trials)\n              if parallel else\n              _run_serial(n_doors, n_trials))\n    return {\"noswitch\": ns, \"switch\": sw}\n\nIt wins when n_trials is very large (e.g., 10–100M) where loop overhead is amortized and you give it many threads, or the design avoids massive temporaries (not much in this example).\nLet’s see their time comparison.\n\nimport timeit\n\n# --- Timing function ---\ndef benchmark(n_doors: int = 3, n_trials: int = 1_000_00, seed: int = 42) -&gt; None:\n    print(f\"n_doors={n_doors}, n_trials={n_trials}\")\n    # Ensure deterministic where possible\n    np.random.seed(seed)\n\n    # Time original\n    t_orig = min(timeit.repeat(lambda: montyhall(n_doors, n_trials),\n                               repeat=3, number=1))\n    print(f\"Original (lists): {t_orig:.4f}s\")\n\n    # Time vectorized\n    t_vec = min(timeit.repeat(lambda: montyhall_fast(n_doors, n_trials, seed),\n                              repeat=3, number=1))\n    print(f\"Vectorized NumPy: {t_vec:.4f}s\")\n\n    # Time Numba serial (compile excluded by earlier warm-up)\n    t_numba_ser = min(timeit.repeat(lambda: montyhall_numba(n_doors, n_trials,\n                                    seed, parallel=False), repeat=3, number=1))\n    print(f\"Numba serial:     {t_numba_ser:.4f}s\")\n\n    # Time Numba parallel\n    t_numba_par = min(timeit.repeat(lambda: montyhall_numba(n_doors, n_trials,\n                                    seed, parallel=True), repeat=3, number=1))\n    print(f\"Numba parallel:   {t_numba_par:.4f}s\")\n\n\nbenchmark(n_doors = 4, n_trials = 1000_000)\n\nn_doors=4, n_trials=1000000\nOriginal (lists): 6.3953s\nVectorized NumPy: 0.0259s\nNumba serial:     0.0163s\nNumba parallel:   0.0029s",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#variables-versus-objects",
    "href": "04-python.html#variables-versus-objects",
    "title": "4  Python Refreshment",
    "section": "4.5 Variables versus Objects",
    "text": "4.5 Variables versus Objects\nIn Python, variables and the objects they point to actually live in two different places in the computer memory. Think of variables as pointers to the objects they’re associated with, rather than being those objects. This matters when multiple variables point to the same object.\n\nx = [1, 2, 3]  # create a list; x points to the list\ny = x          # y also points to the same list in the memory\ny.append(4)    # append to y\nx              # x changed!\n\n[1, 2, 3, 4]\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n5210734528\n5210734528\n\n\nNonetheless, some data types in Python are “immutable”, meaning that their values cannot be changed in place. One such example is strings.\n\nx = \"abc\"\ny = x\ny = \"xyz\"\nx\n\n'abc'\n\n\nNow check their addresses\n\nprint(id(x))   # address of x\nprint(id(y))   # address of y\n\n4311733520\n4910217456\n\n\nQuestion: What’s mutable and what’s immutable?\nAnything that is a collection of other objects is mutable, except tuples.\nNot all manipulations of mutable objects change the object rather than create a new object. Sometimes when you do something to a mutable object, you get back a new object. Manipulations that change an existing object, rather than create a new one, are referred to as “in-place mutations” or just “mutations.” So:\n\nAll manipulations of immutable types create new objects.\nSome manipulations of mutable types create new objects.\n\nDifferent variables may all be pointing at the same object is preserved through function calls (a behavior known as “pass by object-reference”). So if you pass a list to a function, and that function manipulates that list using an in-place mutation, that change will affect any variable that was pointing to that same object outside the function.\n\nx = [1, 2, 3]\ny = x\n\ndef append_42(input_list):\n    input_list.append(42)\n    return input_list\n\nappend_42(x)\n\n[1, 2, 3, 42]\n\n\nNote that both x and y have been appended by \\(42\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#number-representation",
    "href": "04-python.html#number-representation",
    "title": "4  Python Refreshment",
    "section": "4.6 Number Representation",
    "text": "4.6 Number Representation\nNumers in a computer’s memory are represented by binary styles (on and off of bits).\n\n4.6.1 Integers\nIf not careful, It is easy to be bitten by overflow with integers when using Numpy and Pandas in Python.\n\nimport numpy as np\n\nx = np.array(2 ** 63 - 1 , dtype = 'int')\nx\n# This should be the largest number numpy can display, with\n# the default int8 type (64 bits)\n\narray(9223372036854775807)\n\n\nNote: on Windows and other platforms, dtype = 'int' may have to be changed to dtype = np.int64 for the code to execute. Source: Stackoverflow\nWhat if we increment it by 1?\n\ny = np.array(x + 1, dtype = 'int')\ny\n# Because of the overflow, it becomes negative!\n\narray(-9223372036854775808)\n\n\nFor vanilla Python, the overflow errors are checked and more digits are allocated when needed, at the cost of being slow.\n\n2 ** 63 * 1000\n\n9223372036854775808000\n\n\nThis number is 1000 times larger than the prior number, but still displayed perfectly without any overflows.\n\n\n4.6.2 Floating Number\nStandard double-precision floating point number uses 64 bits. Among them, 1 is for sign, 11 is for exponent, and 52 are fraction significand, See https://en.wikipedia.org/wiki/Double-precision_floating-point_format. The bottom line is that, of course, not every real number is exactly representable.\nIf you have played the Game 24, here is a tricky one:\n\n8 / (3 - 8 / 3) == 24\n\nFalse\n\n\nSurprise?\nThere are more.\n\n0.1 + 0.1 + 0.1 == 0.3\n\nFalse\n\n\n\n0.3 - 0.2 == 0.1\n\nFalse\n\n\nWhat is really going on?\n\nimport decimal\ndecimal.Decimal(0.1)\n\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n\n\n\ndecimal.Decimal(8 / (3 - 8 / 3))\n\nDecimal('23.999999999999989341858963598497211933135986328125')\n\n\nBecause the mantissa bits are limited, it can not represent a floating point that’s both very big and very precise. Most computers can represent all integers up to \\(2^{53}\\), after that it starts skipping numbers.\n\n2.1 ** 53 + 1 == 2.1 ** 53\n\n# Find a number larger than 2 to the 53rd\n\nTrue\n\n\n\nx = 2.1 ** 53\nfor i in range(1000000):\n    x = x + 1\nx == 2.1 ** 53\n\nTrue\n\n\nWe add 1 to x by 1000000 times, but it still equal to its initial value, 2.1 ** 53. This is because this number is too big that computer can’t handle it with precision like add 1.\nMachine epsilon is the smallest positive floating-point number x such that 1 + x != 1.\n\nprint(np.finfo(float).eps)\nprint(np.finfo(np.float32).eps)\n\n2.220446049250313e-16\n1.1920929e-07",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "04-python.html#sec-python-venv",
    "href": "04-python.html#sec-python-venv",
    "title": "4  Python Refreshment",
    "section": "4.7 Virtual Environment",
    "text": "4.7 Virtual Environment\nVirtual environments in Python are essential tools for managing dependencies and ensuring consistency across projects. They allow you to create isolated environments for each project, with its own set of installed packages, separate from the global Python installation. This isolation prevents conflicts between project dependencies and versions, making your projects more reliable and easier to manage. It’s particularly useful when working on multiple projects with differing requirements, or when collaborating with others who may have different setups.\nTo set up a virtual environment, you first need to ensure that Python is installed on your system. Most modern Python installations come with the venv module, which is used to create virtual environments. Here’s how to set one up:\n\nOpen your command line interface.\nNavigate to your project directory.\nRun python3 -m venv myenv, where myenv is the name of the virtual environment to be created. Choose an informative name.\n\nThis command creates a new directory named myenv (or your chosen name) in your project directory, containing the virtual environment.\nTo start using this environment, you need to activate it. The activation command varies depending on your operating system:\n\nOn Windows, run myenv\\Scripts\\activate.\nOn Linux or MacOS, use source myenv/bin/activate or . myenv/bin/activate.\n\nOnce activated, your command line will typically show the name of the virtual environment, and you can then install and use packages within this isolated environment without affecting your global Python setup.\nTo exit the virtual environment, simply type deactivate in your command line. This will return you to your system’s global Python environment.\nAs an example, let’s install a package, like numpy, in this newly created virtual environment:\n\nEnsure your virtual environment is activated.\nRun pip install numpy.\n\nThis command installs the requests library in your virtual environment. You can verify the installation by running pip list, which should show requests along with its version.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Python Refreshment</span>"
    ]
  },
  {
    "objectID": "05-ethics_communiacation.html",
    "href": "05-ethics_communiacation.html",
    "title": "5  Data Science Ethics and Communication",
    "section": "",
    "text": "5.1 Data Science Ethics",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science Ethics and Communication</span>"
    ]
  },
  {
    "objectID": "05-ethics_communiacation.html#data-science-ethics",
    "href": "05-ethics_communiacation.html#data-science-ethics",
    "title": "5  Data Science Ethics and Communication",
    "section": "",
    "text": "5.1.1 Introduction\nEthics in data science is a fundamental consideration throughout the lifecycle of any project. Data science ethics refers to the principles and practices that guide responsible and fair use of data to ensure that individual rights are respected, societal welfare is prioritized, and harmful outcomes are avoided. Ethical frameworks like the Belmont Report (Protection of Human Subjects of Biomedical & Research, 1979)} and regulations such as the Health Insurance Portability and Accountability Act (HIPAA) (Health & Services, 1996) have established foundational principles that inspire ethical considerations in research and data use. This section explores key principles of ethical data science and provides guidance on implementing these principles in practice.\n\n\n5.1.2 Principles of Ethical Data Science\n\n5.1.2.1 Respect for Privacy\nSafeguarding privacy is critical in data science. Projects should comply with data protection regulations, such as the General Data Protection Regulation (GDPR) or the California Consumer Privacy Act (CCPA). Techniques like anonymization and pseudonymization must be applied to protect sensitive information. Beyond legal compliance, data scientists should consider the ethical implications of using personal data.\nThe principles established by the Belmont Report emphasize respect for persons, which aligns with safeguarding individual privacy. Protecting privacy also involves limiting data collection to what is strictly necessary. Minimizing the use of identifiable information and implementing secure data storage practices are essential steps. Transparency about how data is used further builds trust with stakeholders.\n\n\n5.1.2.2 Commitment to Fairness\nBias can arise at any stage of the data science pipeline, from data collection to algorithm development. Ethical practice requires actively identifying and addressing biases to prevent harm to underrepresented groups. Fairness should guide the design and deployment of models, ensuring equitable treatment across diverse populations.\nTo achieve fairness, data scientists must assess datasets for representativeness and use tools to detect potential biases. Regular evaluation of model outcomes against fairness metrics helps ensure that systems remain non-discriminatory. The Americans with Disabilities Act (ADA) (Congress, 1990) provides a legal framework emphasizing equitable access, which can inspire fairness in algorithmic design. Collaborating with domain experts and stakeholders can provide additional insights into fairness issues.\n\n\n5.1.2.3 Emphasis on Transparency\nTransparency builds trust and accountability in data science. Models should be interpretable, with clear documentation explaining their design, assumptions, and decision-making processes. Data scientists must communicate results in a way that stakeholders can understand, avoiding unnecessary complexity or obfuscation.\nTransparent practices include providing stakeholders access to relevant information about model performance and limitations. The Federal Data Strategy (Team, 2019) calls for transparency in public sector data use, offering inspiration for practices in broader contexts. Visualizing decision pathways and using tools like LIME or SHAP can enhance interpretability. Establishing clear communication protocols ensures that non-technical audiences can engage with the findings effectively.\n\n\n5.1.2.4 Focus on Social Responsibility\nData science projects must align with ethical goals and anticipate their broader societal and environmental impacts. This includes considering how outputs may be used or misused and avoiding harm to vulnerable populations. Data scientists should aim to use their expertise to promote public welfare, addressing critical societal challenges such as health disparities, climate change, and education access.\nEngaging with diverse perspectives helps align projects with societal values. Ethical codes, such as those from the Association for Computing Machinery (ACM) (Computing Machinery (ACM), 2018), offer guidance on using technology for social good. Collaborating with policymakers and community representatives ensures that data-driven initiatives address real needs and avoid unintended consequences. Regular impact assessments help measure whether projects meet their ethical objectives.\n\n\n5.1.2.5 Adherence to Professional Integrity\nProfessional integrity underpins all ethical practices in data science. Adhering to established ethical guidelines, such as those from the American Statistical Association (ASA) (American Statistical Association (ASA), 2018), ensures accountability. Practices like maintaining informed consent, avoiding data manipulation, and upholding rigor in analyses are essential for maintaining public trust in the field.\nEthical integrity also involves fostering a culture of honesty and openness within data science teams. Peer review and independent validation of findings can help identify potential errors or biases. Documenting methodologies and maintaining transparency in reporting further strengthen trust.\n\n\n\n5.1.3 Ensuring Ethics in Practice\n\n5.1.3.1 Building Ethical Awareness\nPromoting ethical awareness begins with education and training. Institutions should integrate ethics into data science curricula, emphasizing real-world scenarios and decision-making. Organizations should conduct regular training to ensure their teams remain informed about emerging ethical challenges.\nWorkshops and case studies can help data scientists understand the complexities of ethical decision-making. Providing access to resources, such as ethical guidelines and tools, supports continuous learning. Leadership support is critical for embedding ethics into organizational culture.\n\n\n5.1.3.2 Embedding Ethics in Workflows\nEthics must be embedded into every stage of the data science pipeline. Establishing frameworks for ethical review, such as ethics boards or peer-review processes, helps identify potential issues early. Tools for bias detection, explainability, and privacy protection should be standard components of workflows.\nStandard operating procedures for ethical reviews can formalize the consideration of ethics in project planning. Developing templates for documenting ethical decisions ensures consistency and accountability. Collaboration across teams enhances the ability to address ethical challenges comprehensively.\n\n\n5.1.3.3 Establishing Accountability Mechanisms\nClear accountability mechanisms are essential for ethical governance. This includes maintaining documentation for all decisions, establishing audit trails, and assigning responsibility for the outputs of data-driven systems. Organizations should encourage open dialogue about ethical concerns and support whistleblowers who raise issues.\nPeriodic audits of data science projects help ensure compliance with ethical standards. Organizations can benefit from external reviews to identify blind spots and improve their practices. Accountability fosters trust and aligns teams with ethical objectives.\n\n\n5.1.3.4 Engaging Stakeholders\nEthical data science requires collaboration with diverse stakeholders. Including perspectives from affected communities, policymakers, and interdisciplinary experts ensures that projects address real needs and avoid unintended consequences. Stakeholder engagement fosters trust and aligns projects with societal values.\nPublic consultations and focus groups can provide valuable feedback on the potential impacts of data science projects. Engaging with regulators and advocacy groups helps align projects with legal and ethical expectations. Transparent communication with stakeholders builds long-term relationships.\n\n\n5.1.3.5 Continuous Improvement\nEthics in data science is not static; it evolves with technology and societal expectations. Continuous improvement requires regular review of ethical practices, learning from past projects, and adapting to new challenges. Organizations should foster a culture of reflection and growth to remain aligned with ethical best practices.\nEstablishing mechanisms for feedback on ethical practices can identify areas for development. Sharing lessons learned through conferences and publications helps the broader community advance its understanding of ethics in data science.\n\n\n\n5.1.4 Conclusion\nData science ethics is a dynamic and integral aspect of the discipline. By adhering to principles of privacy, fairness, transparency, social responsibility, and integrity, data scientists can ensure their work contributes positively to society. Implementing these principles through structured workflows, stakeholder engagement, and continuous improvement establishes a foundation for trustworthy and impactful data science.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science Ethics and Communication</span>"
    ]
  },
  {
    "objectID": "05-ethics_communiacation.html#effective-data-science-communication",
    "href": "05-ethics_communiacation.html#effective-data-science-communication",
    "title": "5  Data Science Ethics and Communication",
    "section": "5.2 Effective Data Science Communication",
    "text": "5.2 Effective Data Science Communication\nThis section is by Abby White, a senior majoring in Statistical Data Science with a concentration in Advanced Statistics.\n\n5.2.1 Introduction\nData science communication is about more than presenting results; it is about helping others understand and act on them. A clear visualization, a well-phrased sentence, or a reproducible workflow can determine whether your analysis makes an impact or gets lost in translation.\nIn this presentation, I will discuss:\n1. Why communication matters in data science.\n2. Key principles for clarity and transparency.\n3. How visualization and storytelling improve understanding.\n4. The role of reproducibility and ethics in reporting results.\n5. How to connect insights to action.\n\n\n5.2.2 Why Communication Matters\nEven the best analysis doesn’t mean much if people cannot understand it. Clarity, honesty, and reproducibility are what make results useful.\n\n5.2.2.1 Example: Framing a Finding Clearly\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the cleaned NYC crash data\ncrash_df = pd.read_feather(\"../ids-s26/data/nyc_crashes_cleaned.feather\")\n\n# Count crashes by borough\nborough_counts = crash_df[\"borough\"].value_counts().reset_index()\nborough_counts.columns = [\"borough\", \"crash_count\"]\n\nsns.barplot(\n    data=borough_counts,\n    x=\"borough\", y=\"crash_count\",\n    order=borough_counts.sort_values(\"crash_count\", ascending=False)[\"borough\"],\n)\nplt.title(\"NYC Motor Vehicle Collisions by Borough (Labor Day Week, 2025)\")\nplt.xlabel(\"Borough\")\nplt.ylabel(\"Crash Count\")\n\n# Add labels above bars\nfor i, val in enumerate(borough_counts[\"crash_count\"]):\n    plt.text(i, val + (0.01 * borough_counts[\"crash_count\"].max()),\n             f\"{val:,}\", ha=\"center\", fontsize=9)\n\n# Add 5% headroom above the tallest bar\nplt.ylim(0, borough_counts[\"crash_count\"].max() * 1.05)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis bar chart shows total crashes by borough. Without labels, the pattern is visible but vague. Adding numbers and sorting by count makes the trend clear. Brooklyn and Queens lead due to population and road density.\nVisuals that highlight why something happens, not just that it happens, communicate far more effectively.\n\nPoor phrasing:\n“Brooklyn has the highest number of crashes.”\n\n\nBetter phrasing:\n“During Labor Day week 2025, Brooklyn recorded the most motor vehicle collisions, followed by Queens and Manhattan. This pattern likely reflects each borough’s larger population, higher traffic volume, and denser road networks.”\n\nThe first sentence is technically correct but empty of insight. The second version connects data to why it matters. It is specific, comparative, and interpretable.\nGood communication translates data into meaning. When presenting findings, always lead with a clear takeaway before showing details.\n\n\n\n5.2.3 Clarity and Context\nClarity comes from balancing accuracy with accessibility. The goal is to make complex analysis understandable without oversimplifying it.\nGuidelines for clear communication:\n- Lead with the main takeaway before showing details.\n- Define all technical terms (e.g., “injury severity index”).\n- Provide relative comparisons, not just counts.\n- Add short interpretations under visuals.\n\n5.2.3.1 Example: Turning Output into Insight\n\n# Calculate the average number of people injured per crash by borough\ninjury_summary = (\n    crash_df.groupby(\"borough\")[\"number_of_persons_injured\"]\n    .mean()\n    .sort_values()\n)\ninjury_summary\n\nborough\nSTATEN ISLAND    0.562500\nMANHATTAN        0.620087\nQUEENS           0.634146\nBROOKLYN         0.664557\nBRONX            0.800000\nName: number_of_persons_injured, dtype: float64\n\n\n\nTechnical phrasing:\n“Mean injuries per crash differ by borough.”\n\n\nClear phrasing:\n“On average, crashes in the Bronx result in the highest injury rates (about 0.8 people injured per crash), while Staten Island has the lowest (around 0.56). Differences are modest but suggest slightly greater crash severity in more densely populated areas.”\n\nThe first phrasing is correct but vague. It does not tell the audience how or by how much boroughs differ. The clearer version adds real numbers, ranks, and a hint of interpretation (density and traffic volume). Good phrasing gives context so the audience immediately grasps what the data shows.\nTo make this pattern easier to see visually:\n\nsns.barplot(\n    x=injury_summary.values,\n    y=injury_summary.index,\n    color=\"steelblue\"  # same color for all bars\n)\nplt.title(\"Average Injuries per Crash by Borough\")\nplt.xlabel(\"Average Number of Injured People\")\nplt.ylabel(\"Borough\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThe bar chart shows that the Bronx leads with the highest average injuries per crash, while Staten Island has the lowest. Clear labeling, ordered categories, and a clean layout make the trend easy to interpret without visually exaggerating small differences.\n\n\n\n5.2.4 Visual Storytelling\nVisuals are often the clearest way to communicate a pattern. Strong visual design helps people notice relationships quickly and remember them longer.\nA good graphic should make the takeaway obvious in a few seconds. Titles, captions, and color choices all guide the audience toward what matters most.\n\n5.2.4.1 Example: Highlighting a Trend\n\n# Make sure datetime is parsed\ncrash_df[\"crash_datetime\"] = pd.to_datetime(\n    crash_df[\"crash_datetime\"], errors=\"coerce\"\n)\n\n# Group by day \ndaily = (\n    crash_df.groupby(crash_df[\"crash_datetime\"].dt.date)\n    .size()\n    .reset_index(name=\"count\")\n    .rename(columns={\"crash_datetime\": \"date\"})\n)\n\n# Convert date column back to datetime64[ns] for plotting\ndaily[\"date\"] = pd.to_datetime(daily[\"date\"])\n\nsns.lineplot(\n    data=daily.sort_values(\"date\"),\n    x=\"date\",\n    y=\"count\",\n    marker=\"o\"\n)\n\nplt.title(\"Daily NYC Crashes (Labor Day Week, 2025)\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Crash Count\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis line chart shows how crash frequency changed across Labor Day week 2025. The dataset covers a short time window, so we see day-to-day variation rather than long-term trends. Clear labeling and a simple layout make the pattern easy to interpret without overstating the limited time frame.\nA clear title, readable x-axis, and minimal clutter make the trend easy to interpret.\n\nA short caption might read:\n“Daily crash counts fluctuate slightly over the observed period, reflecting normal day-to-day variation in traffic volume.”\n\nThat single sentence turns a small snapshot of data into an understandable story without overstating what the time range can show. Once we understand how visuals convey meaning, the next step is choosing the chart type that best fits our data.\n\n\n\n5.2.5 Choosing the Right Chart Type\nSelecting the right chart is just as important as making it look clean. The wrong type can hide a pattern or even mislead the audience, while the right one highlights exactly what matters.\nWhen deciding how to visualize data, start with the question you want to answer. Every chart should answer one specific question rather than trying to show everything at once. Different chart types serve different purposes:\nCompare Categories:\n- Chart: Bar or column chart\n- Example: Average injuries per crash by borough shows differences clearly without exaggeration.\nShow Change Over Time:\n- Chart: Line chart\n- Example: Daily or hourly crash counts reveal rush-hour spikes or weekend dips.\nDisplay Proportions:\n- Chart: Pie or stacked bar chart\n- Example: Percent of crashes involving pedestrians vs. motorists shows relative risk.\nReveal Relationships:\n- Chart: Scatter plot\n- Example: Plotting vehicle speed vs. injury severity could show how risk increases with speed.\nShow Distributions:\n- Chart: Histogram or box plot\n- Example: A histogram of crash times shows when collisions are most common across a day.\nThese choices matter because each chart highlights a specific relationship between variables. For instance, the line chart of hourly injuries works better than a bar chart because it emphasizes flow and continuity across time. Conversely, comparing borough averages suits a bar chart since categories are discrete.\nWhen in doubt, simplicity and intent guide good design. Avoid flashy visuals or 3D effects that distract from the message. Instead, use consistent colors, clear labels, and honest axes to help the audience see what you want them to see.\n\n\n5.2.6 Reproducibility and Transparency\nReproducibility builds trust in your analysis. Transparent workflows including data sources, software versions, and assumptions, allow others to verify and extend your work. In data science, reproducibility isn’t just about rerunning code; it’s about clear communication of process.\nGood practices for transparency:\n- Combine code and writing in a single Quarto or Jupyter file.\n- Record where and when data was retrieved.\n- Comment on all major data-cleaning steps.\n- Use readable variable names and consistent file structure.\n\n5.2.6.1 Example: Adding Reproducible Metadata\n\n# Data source: NYC Open Data (accessed October 2025)\n# File: ids-f25/data/nyc_crashes_cleaned.feather\n# Environment: Python 3.11 | pandas 2.2 | seaborn 0.13\n\nprint(crash_df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1381 entries, 0 to 1380\nData columns (total 30 columns):\n #   Column                         Non-Null Count  Dtype         \n---  ------                         --------------  -----         \n 0   borough                        1367 non-null   object        \n 1   zip_code                       1365 non-null   object        \n 2   latitude                       1345 non-null   float32       \n 3   longitude                      1345 non-null   float32       \n 4   on_street_name                 953 non-null    object        \n 5   cross_street_name              839 non-null    object        \n 6   off_street_name                428 non-null    object        \n 7   number_of_persons_injured      1381 non-null   int64         \n 8   number_of_persons_killed       1381 non-null   int64         \n 9   number_of_pedestrians_injured  1381 non-null   int64         \n 10  number_of_pedestrians_killed   1381 non-null   int64         \n 11  number_of_cyclist_injured      1381 non-null   int64         \n 12  number_of_cyclist_killed       1381 non-null   int64         \n 13  number_of_motorist_injured     1381 non-null   int64         \n 14  number_of_motorist_killed      1381 non-null   int64         \n 15  contributing_factor_vehicle_1  1372 non-null   object        \n 16  contributing_factor_vehicle_2  1059 non-null   object        \n 17  contributing_factor_vehicle_3  118 non-null    object        \n 18  contributing_factor_vehicle_4  33 non-null     object        \n 19  contributing_factor_vehicle_5  12 non-null     object        \n 20  collision_id                   1381 non-null   int64         \n 21  vehicle_type_code_1            1364 non-null   object        \n 22  vehicle_type_code_2            945 non-null    object        \n 23  vehicle_type_code_3            112 non-null    object        \n 24  vehicle_type_code_4            30 non-null     object        \n 25  vehicle_type_code_5            12 non-null     object        \n 26  was_fillable                   1381 non-null   bool          \n 27  zip_code_numeric               1365 non-null   float64       \n 28  zip_filled                     1381 non-null   bool          \n 29  crash_datetime                 1381 non-null   datetime64[ns]\ndtypes: bool(2), datetime64[ns](1), float32(2), float64(1), int64(9), object(15)\nmemory usage: 304.8+ KB\nNone\n\n\nIncluding the info() output and environment details helps document your data’s structure. Anyone revisiting your project can quickly see how the dataset was formatted and what columns were used. This kind of metadata makes collaboration and replication straightforward.\n\n\n\n5.2.7 Ethical and Responsible Communication\nEthical communication means presenting your data truthfully and clearly. When data visuals or summaries are misleading, even unintentionally, they can distort public understanding. Effective communicators focus on honesty and clarity.\nCommon pitfalls:\n- Cropping or compressing axes to exaggerate trends.\n- Omitting missing data or uncertainty.\n- Implying causation when showing correlation.\nBetter habits:\n- Start axes at zero when showing counts.\n- Provide notes or error ranges when possible.\n- Write captions that clarify context and limitations.\n\n5.2.7.1 Example 1: Cropped Axis\n\n# Example 1: Cropped Axis\nymin = borough_counts[\"crash_count\"].min()\nymax = borough_counts[\"crash_count\"].max()\nsecond_lowest = sorted(borough_counts[\"crash_count\"])[1]\n\nfig, axes = plt.subplots(2, 1, figsize=(7, 7))\n\n# Misleading chart (top)\nsns.barplot(\n    data=borough_counts,\n    x=\"borough\",\n    y=\"crash_count\",\n    ax=axes[0],\n    color=\"steelblue\"\n)\naxes[0].set_ylim(second_lowest * 0.9, ymax * 1.02)\naxes[0].set_title(\"Misleading Chart: Axis Cropped Too High\")\naxes[0].set_xlabel(\"\")\naxes[0].set_ylabel(\"Crash Count\")\naxes[0].tick_params(axis=\"x\", rotation=45)\n\n# Honest chart (bottom)\nsns.barplot(\n    data=borough_counts,\n    x=\"borough\",\n    y=\"crash_count\",\n    ax=axes[1],\n    color=\"steelblue\"\n)\naxes[1].set_ylim(0, ymax * 1.02)\naxes[1].set_title(\"Honest Chart: Axis Starts at Zero\")\naxes[1].set_xlabel(\"Borough\")\naxes[1].set_ylabel(\"Crash Count\")\naxes[1].tick_params(axis=\"x\", rotation=45)\n\nplt.tight_layout(pad=2)\nplt.show()\n\n\n\n\n\n\n\n\nWhen the y-axis is cropped so it starts just below the smaller bars, Brooklyn’s crash count looks dramatically higher than the other boroughs. The honest version, which starts the y-axis at zero, shows that the differences are real but not as extreme. This demonstrates how axis limits can exaggerate scale, even when the underlying data is unchanged.\n\n\n5.2.7.2 Example 2: Distorted Aspect Ratio\n\n# Example 2: Distorted Aspect Ratio\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5))\n\nsns.lineplot(x=[1, 2, 3, 4, 5], y=[100, 200, 300, 400, 500],\n             ax=axes[0], marker=\"o\")\n\naxes[0].set_box_aspect(1.8)  # misleading proportions\naxes[0].set_title(\"Misleading: Distorted Aspect Ratio\")\naxes[0].set_xlabel(\"Time\")\naxes[0].set_ylabel(\"Value\")\n\nsns.lineplot(x=[1, 2, 3, 4, 5], y=[100, 200, 300, 400, 500],\n             ax=axes[1], marker=\"o\")\naxes[1].set_box_aspect(1)  # normal proportions\naxes[1].set_title(\"Honest: Equal Scaling\")\naxes[1].set_xlabel(\"Time\")\naxes[1].set_ylabel(\"Value\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nChanging the aspect ratio alters the apparent steepness of trends. The left plot exaggerates change by compressing the x-axis, while the right plot uses consistent scaling to display the real rate of change. Aspect distortion is subtle but powerful, it can make normal variation seem dramatic.\n\n\n5.2.7.3 Example 3: Misleading Color Emphasis\n\n# Example 3: Misleading Color Emphasis\n\n# Sample data \ndata = pd.DataFrame({\n    \"borough\": [\"Brooklyn\", \"Queens\", \"Manhattan\", \"Bronx\"],\n    \"crash_rate\": [8.1, 7.9, 7.7, 7.5]\n})\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 3))\n\n# Misleading chart: overemphasized colors \nsns.barplot(\n    data=data,\n    x=\"borough\",\n    y=\"crash_rate\",\n    hue=\"borough\",\n    palette=[\"darkred\", \"red\", \"salmon\", \"pink\"],\n    legend=False,\n    ax=axes[0]\n)\naxes[0].set_title(\"Misleading: Color Overemphasis\")\n\n# Honest chart: consistent, neutral color\nsns.barplot(\n    data=data,\n    x=\"borough\",\n    y=\"crash_rate\",\n    color=\"steelblue\",\n    ax=axes[1]\n)\naxes[1].set_title(\"Honest: Neutral Colors\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nColor choices can easily mislead. The chart on the left uses intense red tones to suggest large differences between boroughs, even though the variation is small. The right chart uses a neutral palette to draw attention to data values instead of emotional cues.\nEthical visualization respects the audience’s ability to interpret data fairly. This connects to the broader issue of abusing a plot which involves using design choices to distort perception. Such misuse can involve altering aspect ratios, exaggerating colors, omitting context, or adjusting baselines to amplify change. True integrity in visualization means clarity over drama. We should show data as it is, not as we wish it looked.\n\n\n\n5.2.8 Connecting Insights to Action\nGreat communication doesn’t stop at describing what happened, it explains what should happen next. Visuals paired with short, concrete takeaways make insights actionable.\n\n5.2.8.1 Example: Turning Findings into Decisions\n\n# Filter crashes with any injuries (pedestrians, cyclists, or motorists)\ninjury_df = crash_df[\n    (crash_df[\"number_of_pedestrians_injured\"] &gt; 0)\n    | (crash_df[\"number_of_cyclist_injured\"] &gt; 0)\n    | (crash_df[\"number_of_motorist_injured\"] &gt; 0)\n].copy()\n\n# Extract hour of day\ninjury_df[\"hour\"] = injury_df[\"crash_datetime\"].dt.hour\n\n# Create a combined total injury column\ninjury_df[\"total_injuries\"] = (\n    injury_df[\"number_of_pedestrians_injured\"]\n    + injury_df[\"number_of_cyclist_injured\"]\n    + injury_df[\"number_of_motorist_injured\"]\n)\n\n# Group by borough and hour\ninjuries_by_hour = (\n    injury_df.groupby([\"borough\", \"hour\"])[\"total_injuries\"]\n    .sum()\n    .reset_index()\n)\n\n# Plot total injuries by hour for each borough\nsns.lineplot(\n    data=injuries_by_hour,\n    x=\"hour\",\n    y=\"total_injuries\",\n    hue=\"borough\",\n    marker=\"o\"\n)\nplt.title(\"Hourly Injuries (Pedestrians, Cyclists, and Motorists) by Borough\")\nplt.xlabel(\"Hour of Day\")\nplt.ylabel(\"Total Injuries\")\nplt.xticks(range(0, 24, 2))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot shows how total traffic-related injuries (pedestrians, cyclists, and motorists combined) change by hour in each borough. Injuries stay relatively low overnight, then rise during the afternoon and peak in the late day and early evening; most noticeably in Brooklyn and Queens, with a smaller but similar bump in the Bronx and Manhattan. Staten Island stays consistently lower overall. These rush-hour spikes line up with high activity on the roads where there are more cars, more people moving, and more chances for conflict. That suggests a clear intervention window.\n\n“Injuries across Brooklyn and Queens jump in the late afternoon and evening, which lines up with commuting traffic. Targeted enforcement, traffic calming, and signal timing changes during these peak hours could help reduce harm.”\n\nHere, the analysis leads directly to a recommendation. That bridge from insight to action turns analysis into impact. Communicating findings this way helps decision-makers use data effectively.\n\n\n\n5.2.9 Recommendations for Effective Communication and Presentation\nGood data communication does not end with a well-designed chart. It extends to how findings are framed, timed, and delivered. Clear, focused communication helps turn technical results into insights that people can actually use.\n\n5.2.9.1 General Recommendations\n\nLead with purpose by explaining why the data matters before discussing details.\n\nShow the story, not the spreadsheet, visuals should clarify, not overwhelm.\n\nKeep design elements consistent so colors, scales, and fonts feel cohesive.\n\nAnticipate how non-technical audiences might interpret results and add context when needed.\n\nConnect each chart or finding to its real-world relevance or next steps.\n\n\n\n5.2.9.2 Being Time-Aware\n\nKnow your total time and plan the pacing of your talk accordingly.\n\nPrioritize the most important findings, it’s better to explain a few points clearly than to rush through many.\n\nPractice transitions between sections so the flow feels natural and on schedule.\n\nKeep visuals simple so they can be understood quickly without overexplaining.\n\nLeave a few minutes for questions or discussion at the end if possible.\n\nIf you run short on time, skip details that don’t change the main takeaway.\n\n\n\n5.2.9.3 Giving a Strong Presentation\n\nStart with a clear message so your audience knows what to expect.\n\nGuide the audience through each visual: what to notice and why it matters.\n\nKeep slides uncluttered, focusing on one key idea at a time.\n\nSpeak at a steady pace, pausing briefly after key visuals to let points sink in.\n\nAvoid jargon and tailor explanations to your audience’s background.\n\nEnd with a short summary that reinforces the main insight and next step.\n\nDelivering data effectively means combining clarity, timing, and empathy for your audience. When visuals, pacing, and delivery align, data becomes insight rather than just information.\n\n\n\n5.2.10 Conclusion\nEffective data science communication blends clarity, accuracy, reproducibility, and ethics. When practiced together, these elements transform complex analyses into stories people can understand and act on.\nThe NYC crash dataset shows how simple design, transparent documentation, and ethical framing make results meaningful far beyond the numbers.\n\n\n5.2.11 Further Reading\neazyBI Blog. (2025). Data Visualization: How to Pick the Right Chart Type.\nhttps://eazybi.com/blog/data-visualization-how-to-pick-the-right-chart-type\nFranconeri, S. L., Padilla, L. M. K., Shah, P., Zacks, J. M., & Hullman, J. (2021).\nThe science of visual data communication: What works. Psychological Science in\nthe Public Interest, 22(3), 110-161.\nhttps://faculty.sites.iastate.edu/tesfatsi/archive/tesfatsi/ScienceOfVisualDataCommunication.FranconeriEtAl2021.pdf\nOfori, E., et al. (2025). Visual communication of public health data:\nA scoping review. Public Health Reviews.\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC12060258/\nPragmatic Editorial Team. (2025). Communication Skills for Data Science.\nhttps://www.pragmaticinstitute.com/resources/articles/data/communication-skills-for-data-science/\n\n\n\n\nAmerican Statistical Association (ASA). (2018). Ethical guidelines for statistical practice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and professional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990 (ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance portability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, & Research, B. (1979). The belmont report: Ethical principles and guidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action plan.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Science Ethics and Communication</span>"
    ]
  },
  {
    "objectID": "95-exercises.html",
    "href": "95-exercises.html",
    "title": "6  Homework",
    "section": "",
    "text": "6.1 Homework 1 (due 02/06)\nQuarto and Git setup: Quarto and Git are two important tools for data science: Quarto for communicating results (reports/websites/books) and Git/GitHub for version control and collaboration. This homework is about getting familiar with them through the following tasks.\nYour goal: produce a short, beginner-friendly step-by-step manual for yourself (and future students) that documents what you did, what went wrong, and how you fixed it. Please use the templates/hw.qmd as your starting point. Use the command line interface.\nWork in your GitHub Classroom repository (the private repo created for you when you accept the assignment link). Use the command line interface unless a step explicitly requires a browser (e.g., adding an SSH key).\nSubmission (What should be included in your repo)\nTasks\nIn hw.qmd, record exactly what you installed (version + method), and any issues you hit (PATH problems are common) and how you fixed them.\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"polar\"})\nax.plot(theta, r)\nax.set_rmax(2.0)\nax.grid(True)\nplt.show()\nRender the homework into a HTML. Print the HTML file to a pdf file and put the file into a release in your GitHub repo.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework</span>"
    ]
  },
  {
    "objectID": "95-exercises.html#homework-1-due-0206",
    "href": "95-exercises.html#homework-1-due-0206",
    "title": "6  Homework",
    "section": "",
    "text": "hw.qmd (your completed manual)\nhw.html (rendered from hw.qmd)\nhw.pdf\n\n\n\nAccept the GitHub Classroom assignment and clone your repo (SSH).\n\n\nAccept the assignment using the GitHub Classroom link provided on HuskyCT.\nIn a terminal, verify Git is installed:\n\ngit --version\n\nSet your Git identity (do this once per computer):\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"XXX@uconn.edu\" (use the email tied to your GitHub account if different).\n\n\n\nSet up SSH authentication between your computer and GitHub. There are online tutorials with more details.\n\n\nCheck whether you already have an SSH key: +ls -al ~/.ssh\nIf you do not see something like id_ed25519 and id_ed25519.pub, create a new key:\n\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nPress Enter to accept the default location (~/.ssh/id_ed25519)\nChoose a passphrase or press Enter for none\n\nStart the SSH agent and add your key:\n\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\nCopy your public key and add it to GitHub:\n\ncat ~/.ssh/id_ed25519.pub\nIn your browser: GitHub → Settings → SSH and GPG keys → New SSH key\n\nTest the connection:\n\nssh -T git@github.com\nYou should see a message that you have successfully authenticated.\n\nNow clone your GitHub Classroom repo (SSH) and enter it:\n\ngit clone git@github.com:&lt;ORG&gt;/&lt;REPO&gt;.git\ncd &lt;REPO&gt;\n\n\n\nSet up the homework file in your repo.\n\n\nDownload/copy the template into your Classroom repo as hw.qmd.\nMake an initial commit:\n\ngit status\ngit add hw.qmd\ngit commit -m \"Start HW1\"\ngit push\n\n\n\nInstall Quarto and verify it works. Follow the official installation instructions at https://quarto.org/docs/get-started/ for your operating system. Then verify:\n\nquarto --version\nquarto check\n\n\n\n\nPick an editor/tool and reproduce the “line plot on polar axis” example.\n\n\n\nChoose one tool (e.g., VS Code, Jupyter, Positron, Emacs) and follow the Quarto documentation to reproduce the example plot.\nMinimum requirement: include the plot in your hw.qmd using a Python code cell that produces a polar line plot with Matplotlib. For example:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework</span>"
    ]
  },
  {
    "objectID": "95-exercises.html#homework-2-due-0213",
    "href": "95-exercises.html#homework-2-due-0213",
    "title": "6  Homework",
    "section": "6.2 Homework 2 (due 02/13)",
    "text": "6.2 Homework 2 (due 02/13)\n\n6.2.1 Part 1. Contributing to the Class Notes\nThis part of the homework is to make sure that you know how to contribute to our classnotes by making pull request and to complete updating of your wishlist and presentation topic.\nTo contribute to the classnote repository, you need to have a working copy of the sources on your computer. Document the following steps in a qmd file in the form of a step-by-step manual, as if you are explaining them to someone who wants to contribute too. Make at least 5 commits for this task, each with an informative message.\n\nCreate a fork of the notes repo with your own GitHub account.\nClone it to an appropriate folder on your computer.\nRender the classnotes on your computer. If you encounter difficulties, try to document and resolve the issues (Quarto/Python installation, environment setup/activation, etc.).\n\nMake a new branch (and name it appropriately) to experiment with your changes.\nCheckout your branch and add your wishes to the wish list; check the rendered notes to make sure your changes look good.\nCommit with an informative message; and push the changes to your GitHub account.\nMake a pull request to class notes repo from your fork at GitHub. Make sure you have clear messages to document the changes\nMake another branch and repeat the process to update your presentation topic.\n\nThe grading will be based on whether you can successfully make pull requests to update your wishlist and presentation topic on time.\n\n\n6.2.2 Part 2. Generalized “Fibonacci-like” recurrence\nDefine a sequence \\(\\{a_n\\}\\) by\n\n\\(a_1 = 1, a_2 = 1\\)\n\\(a_n = c_1 a_{n-1} + c_2 a_{n-2}\\) for \\(n \\ge 3\\)\n\nwhere \\(c_1, c_2\\) are integers. Note that Fibonacci sequence corresponds to \\(c_1 = 1, c_2 = 1\\).\n\nBased on the Fibonacci example in the notes, implement four functions:\n\nseq_rs(n, c1, c2) (naive recursion)\nseq_dm(n, c1, c2) (memoization)\nseq_dbu(n, c1, c2) (bottom-up with list)\nseq_dbu_m(n, c1, c2) (bottom-up constant memory)\n\n(optional) Add input checks and raise ValueError with a helpful message if:\n\nn is not an integer or n &lt; 1\nc1 or c2 is not an integer\n\nBenchmark all four methods for:\n\nn = 10, 20, 40\ncoefficients (c1, c2) = (1, 1) and (2, 1)\nUse %timeit or timeit and present results in a small table.\n\nBriefly explain (2–5 sentences):\n\nWhy recursion slows dramatically\nHow memoization changes time complexity\nMemory usage differences between the bottom-up list vs constant-memory version\n\n\n\n\n6.2.3 Part C. Fast method using matrices\nFor the recurrence \\(a_n = c_1 a_{n-1} + c_2 a_{n-2}\\), define the matrix:\n\\[M = \\begin{pmatrix}\nc_1 & c_2 \\\\\n1 & 0\n\\end{pmatrix}\\]\nThen, one can show that:\n\\[\\begin{pmatrix}\na_n \\\\\na_{n-1}\n\\end{pmatrix}\n=\nM^{n-2}\n\\begin{pmatrix}\na_2 \\\\\na_1\n\\end{pmatrix}\n\\quad \\text{for } n \\ge 2\\]\nIt turns out this gives a very efficient way for computing the sequence (in fact the most efficient way with complexity O(log n)).\n\nImplement seq_log(n, c1, c2) that computes (a_n). You may use the matrix functions provided below.\nBenchmark seq_dbu_m vs seq_log for large n (choose values like 10**3, 10**4 or as large as your machine can handle).\nIn a few sentences, write down the biggest take-home message you learn from using five different ways of solving the same problem.\n\n\nRender your homework into a HTML. Print the HTML file to a pdf file and put them together with source files (.qmd) into a release in your GitHub repo.\nExample code: 2×2 matrices + operations\nHere are some functions of matrices that could be useful to you. You may direct use these defined functions in implementing seq_log(n, c1, c2).\n\n# A 2x2 matrix is represented as:\n# [[a, b],\n#  [c, d]]\n\ndef mat2_mul(A, B):\n    \"\"\"Multiply two 2x2 matrices A and B.\"\"\"\n    return [\n        [\n            A[0][0] * B[0][0] + A[0][1] * B[1][0],\n            A[0][0] * B[0][1] + A[0][1] * B[1][1],\n        ],\n        [\n            A[1][0] * B[0][0] + A[1][1] * B[1][0],\n            A[1][0] * B[0][1] + A[1][1] * B[1][1],\n        ],\n    ]\n\ndef mat2_vec_mul(A, v):\n    \"\"\"Multiply 2x2 matrix A by 2-vector v = [x, y].\"\"\"\n    return [\n        A[0][0] * v[0] + A[0][1] * v[1],\n        A[1][0] * v[0] + A[1][1] * v[1],\n    ]\n\ndef mat2_pow(M, n):\n    \"\"\"\n    Compute M^n for a 2x2 matrix M and integer n &gt;= 0\n    using exponentiation by squaring.\n    \"\"\"\n    if not isinstance(n, int) or n &lt; 0:\n        raise ValueError(\"n must be an integer &gt;= 0\")\n\n    # Identity matrix I\n    result = [[1, 0],\n              [0, 1]]\n    base = M\n\n    while n &gt; 0:\n        if n % 2 == 1:\n            result = mat2_mul(result, base)\n        base = mat2_mul(base, base)\n        n //= 2\n\n    return result\n\n\n# Quick sanity checks\nI = [[1, 0], [0, 1]]\nA = [[2, 3], [4, 5]]\n\nassert mat2_mul(I, A) == A\nassert mat2_mul(A, I) == A\nassert mat2_pow(A, 0) == I\nassert mat2_pow(A, 1) == A",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Homework</span>"
    ]
  },
  {
    "objectID": "99-references.html",
    "href": "99-references.html",
    "title": "References",
    "section": "",
    "text": "American Statistical Association (ASA). (2018). Ethical guidelines\nfor statistical practice.\n\n\nComputing Machinery (ACM), A. for. (2018). Code of ethics and\nprofessional conduct.\n\n\nCongress, U. S. (1990). Americans with disabilities act of 1990\n(ADA).\n\n\nHealth, U. S. D. of, & Services, H. (1996). Health insurance\nportability and accountability act of 1996 (HIPAA).\n\n\nProtection of Human Subjects of Biomedical, N. C. for the, &\nResearch, B. (1979). The belmont report: Ethical principles and\nguidelines for the protection of human subjects of research.\n\n\nTeam, F. D. S. D. (2019). Federal data strategy 2020 action\nplan.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.\n\n\nYu, B., & Barter, R. L. (2024). Veridical data science: The\npractice of responsible data analysis and decision making. MIT\nPress.",
    "crumbs": [
      "References"
    ]
  }
]